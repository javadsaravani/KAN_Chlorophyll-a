{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463bee62-8335-4e18-ab17-d908882f4753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import numpy as np\n",
    "import torch\n",
    "from kan import KAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774976f9-8043-464f-a82f-6bfced99b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('file.csv')\n",
    "data['date'] = pd.to_datetime(data['date'], format='%m/%d/%Y')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b74fc67-2831-4f52-a298-16a149fffd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chl_a_values = data[['chl_a']].values\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "data['lof_score'] = lof.fit_predict(chl_a_values)\n",
    "\n",
    "data = data[data['lof_score'] != -1].drop(columns=['lof_score'])\n",
    "forecast_steps = 6  \n",
    "date_range = pd.date_range(start='2002-08-01', end='2024-08-01', freq='MS')\n",
    "data.set_index('date', inplace=True)\n",
    "data = data.reindex(date_range)\n",
    "missing_dates = data[data['chl_a'].isnull()].index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data['chl_a'] = data['chl_a'].fillna(method='ffill')\n",
    "\n",
    "data['chl_a'] = data['chl_a'].fillna(method='bfill')\n",
    "\n",
    "stl = STL(data['chl_a'], seasonal=13)\n",
    "result = stl.fit()\n",
    "\n",
    "data['chl_a'] = np.where(data['chl_a'].isnull(), result.trend + result.seasonal, data['chl_a'])\n",
    "\n",
    "data['chl_a'] = data['chl_a'].interpolate(method='linear')\n",
    "data.to_csv('imputed.csv', index=True)\n",
    "\n",
    "print(data.isna().sum())\n",
    "\n",
    "\n",
    "data['month'] = data.index.month\n",
    "\n",
    "data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)\n",
    "data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)\n",
    "data = data.drop(['month'], axis=1)\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(data.index, data['chl_a'], label='Chl-a Concentration', color='blue')\n",
    "\n",
    "imputed_values = data.loc[missing_dates]\n",
    "plt.scatter(imputed_values.index, imputed_values['chl_a'], color='red', label='Imputed Values')\n",
    "\n",
    "plt.title('Chlorophyll-a Concentration Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Chlorophyll-a (chl_a)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecfc1d5-0e71-464d-bb8a-e05368fd5ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "if forecast_steps > 0:\n",
    "    data_for_forecasting = data.iloc[-forecast_steps:]\n",
    "    data = data.iloc[:-forecast_steps] \n",
    "else:\n",
    "    data = data\n",
    "    data_for_forecasting = pd.DataFrame()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3d768a-50dd-40e1-b19c-6c1761d99e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_pacf(data['chl_a'].dropna(), lags=12, method='ywm')  plt.title('Partial Autocorrelation of Chlorophyll-a')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b530df96-9a09-4b33-93b1-5f9757a25e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b886719-583e-4034-a058-f40ec9c893e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_LSTM = data.copy()\n",
    "data_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b87de4-d298-4adc-9ae3-13dce812c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_r2(y_true, y_pred):\n",
    "    y_true_mean = np.mean(y_true)\n",
    "    y_pred_mean = np.mean(y_pred)\n",
    "    \n",
    "    numerator = np.mean((y_true - y_true_mean) * (y_pred - y_pred_mean))\n",
    "    denominator = np.sqrt(np.mean((y_true - y_true_mean) ** 2) * np.mean((y_pred - y_pred_mean) ** 2))\n",
    "    \n",
    "    r = numerator / denominator\n",
    "    return r ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26060696-95ee-4c63-87a4-35c16efb4984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lag = 12  for i in range(1, lag + 1):\n",
    "    data[f'yt-{i}'] = data['chl_a'].shift(i)\n",
    "\n",
    "data.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce8514-698d-4d59-9b9f-26aa1b98b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bf9417-30e4-464c-aec7-a4c50ee3b1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "X = data[[f'yt-{i}' for i in range(1, lag + 1)]].values\n",
    "y = data['chl_a'].values\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "best_r2 = -np.inf\n",
    "best_model_metrics = None\n",
    "best_train_results_df = None\n",
    "best_test_results_df = None\n",
    "\n",
    "for run in range(20):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=run)\n",
    "\n",
    "        model = LinearRegression()\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "        train_custom_r2 = custom_r2(y_train, y_pred_train)\n",
    "    test_custom_r2 = custom_r2(y_test, y_pred_test)\n",
    "\n",
    "        print(f\"Run {run + 1}: Train R2 = {train_r2:.4f}, Test R2 = {test_r2:.4f}, Custom Test R2 = {test_custom_r2:.4f}\")\n",
    "\n",
    "        if test_custom_r2 > best_r2:\n",
    "        best_r2 = test_custom_r2\n",
    "\n",
    "                train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "        test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "        train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "        test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "                best_train_results_df = pd.DataFrame({\n",
    "            'Actual_Train': y_train.flatten(),\n",
    "            'Predicted_Train': y_pred_train.flatten()\n",
    "        })\n",
    "\n",
    "        best_test_results_df = pd.DataFrame({\n",
    "            'Actual_Test': y_test.flatten(),\n",
    "            'Predicted_Test': y_pred_test.flatten()\n",
    "        })\n",
    "\n",
    "                best_model_metrics = {\n",
    "            'Train MAE': train_mae,\n",
    "            'Train MSE': train_mse,\n",
    "            'Train RMSE': train_rmse,\n",
    "            'Train R2': train_r2,\n",
    "            'Test MAE': test_mae,\n",
    "            'Test MSE': test_mse,\n",
    "            'Test RMSE': test_rmse,\n",
    "            'Test R2': test_r2\n",
    "        }\n",
    "        \n",
    "Act_Train = y_train.flatten()\n",
    "Act_Test = y_test.flatten()\n",
    "MLR_Train = y_pred_train.flatten()\n",
    "MLR_Test = y_pred_test.flatten()\n",
    "\n",
    "with pd.ExcelWriter('MLR.xlsx') as writer:\n",
    "    best_train_results_df.to_excel(writer, sheet_name='Train', index=False)\n",
    "    best_test_results_df.to_excel(writer, sheet_name='Test', index=False)\n",
    "\n",
    "        metrics_df = pd.DataFrame(best_model_metrics, index=[0])\n",
    "    metrics_df.to_excel(writer, sheet_name='Metrics', index=False)\n",
    "\n",
    "print(f\"The best model has been exported to 'MLR_South_Caspian_Best.xlsx' with a custom R2 of {best_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f1e2a0-e944-499e-ace7-87635694798d",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed13215b-a334-46d1-b253-ccae8624144c",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c053bb8-7ebc-4276-87bb-f74defead6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate):\n",
    "        super(ANNModel, self).__init__()\n",
    "        layers = []\n",
    "        current_size = input_size\n",
    "\n",
    "                for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(current_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))              current_size = hidden_size\n",
    "\n",
    "        layers.append(nn.Linear(current_size, output_size))          self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def train_and_evaluate_ann(hidden_sizes, learning_rate, dropout_rate):\n",
    "    model = ANNModel(input_size=5, hidden_sizes=hidden_sizes, output_size=1, dropout_rate=dropout_rate)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        num_epochs = 50\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "                loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions_test = model(X_test_tensor).numpy()\n",
    "        actual_test = y_test_tensor.numpy()\n",
    "\n",
    "        test_r2 = np.corrcoef(actual_test.flatten(), predictions_test.flatten())[0, 1] ** 2\n",
    "    return test_r2\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "hidden_layer_configs = [[32], [64], [32, 64], [64, 128]]  dropout_rates = [0.1, 0.2, 0.3]\n",
    "\n",
    "best_r2 = -float('inf')\n",
    "best_params = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for hidden_sizes in hidden_layer_configs:\n",
    "        for dropout_rate in dropout_rates:\n",
    "            print(f\"Training with LR={lr}, Hidden Sizes={hidden_sizes}, Dropout Rate={dropout_rate}\")\n",
    "            test_r2 = train_and_evaluate_ann(hidden_sizes, lr, dropout_rate)\n",
    "            print(f\"Test R2: {test_r2:.4f}\")\n",
    "\n",
    "            if test_r2 > best_r2:\n",
    "                best_r2 = test_r2\n",
    "                best_params = {'Learning Rate': lr, 'Hidden Sizes': hidden_sizes, 'Dropout Rate': dropout_rate}\n",
    "\n",
    "print(f\"Best Hyperparameters: {best_params} with Test R2: {best_r2:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3574b63f-b90a-4db6-b900-4882edec28b9",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80695a71-5339-4191-9ec6-6f13c5ba697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "X = data[[f'yt-{i}' for i in range(1, lag + 1)] + ['month_sin', 'month_cos']].values\n",
    "y = data['chl_a'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "class ANNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANNModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(14, 32)          self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 64)          self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def custom_r2(y_true, y_pred):\n",
    "    y_true_mean = np.mean(y_true)\n",
    "    y_pred_mean = np.mean(y_pred)\n",
    "    numerator = np.mean((y_true - y_true_mean) * (y_pred - y_pred_mean))\n",
    "    denominator = np.sqrt(np.mean((y_true - y_true_mean) ** 2) * np.mean((y_pred - y_pred_mean) ** 2))\n",
    "    r = numerator / denominator\n",
    "    return r ** 2\n",
    "\n",
    "def train_and_evaluate_ann():\n",
    "    model = ANNModel()\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "                loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions_train = model(X_train_tensor).numpy()\n",
    "        predictions_test = model(X_test_tensor).numpy()\n",
    "        actual_train = y_train_tensor.numpy()\n",
    "        actual_test = y_test_tensor.numpy()\n",
    "\n",
    "        train_r2 = np.corrcoef(actual_train.flatten(), predictions_train.flatten())[0, 1] ** 2\n",
    "    test_r2 = np.corrcoef(actual_test.flatten(), predictions_test.flatten())[0, 1] ** 2\n",
    "\n",
    "        train_custom_r2 = custom_r2(actual_train, predictions_train)\n",
    "    test_custom_r2 = custom_r2(actual_test, predictions_test)\n",
    "\n",
    "        train_corr, train_p_value = pearsonr(actual_train.flatten(), predictions_train.flatten())\n",
    "    test_corr, test_p_value = pearsonr(actual_test.flatten(), predictions_test.flatten())\n",
    "\n",
    "    return (model, train_custom_r2, test_custom_r2, train_r2, test_r2,\n",
    "            train_p_value, test_p_value, predictions_train, actual_train,\n",
    "            predictions_test, actual_test)\n",
    "\n",
    "best_r2 = -float('inf')\n",
    "best_model = None\n",
    "best_metrics = None\n",
    "\n",
    "for i in range(20):\n",
    "    (model, train_custom_r2, test_custom_r2, train_r2, test_r2, train_p_value, \n",
    "     test_p_value, predictions_train, actual_train, predictions_test, actual_test) = train_and_evaluate_ann()\n",
    "\n",
    "    if test_custom_r2 > best_r2:\n",
    "        best_r2 = test_custom_r2\n",
    "        best_model = model\n",
    "        best_metrics = {\n",
    "            'Train Custom R2': train_custom_r2,\n",
    "            'Test Custom R2': test_custom_r2,\n",
    "            'Train R2': train_r2,\n",
    "            'Test R2': test_r2,\n",
    "            'Train p-value': train_p_value,\n",
    "            'Test p-value': test_p_value,\n",
    "            'Train MAE': np.mean(np.abs(predictions_train - actual_train)),\n",
    "            'Test MAE': np.mean(np.abs(predictions_test - actual_test)),\n",
    "            'Train MSE': np.mean((predictions_train - actual_train) ** 2),\n",
    "            'Test MSE': np.mean((predictions_test - actual_test) ** 2),\n",
    "            'Train RMSE': np.sqrt(np.mean((predictions_train - actual_train) ** 2)),\n",
    "            'Test RMSE': np.sqrt(np.mean((predictions_test - actual_test) ** 2)),\n",
    "            'Train Predictions': predictions_train.flatten(),\n",
    "            'Test Predictions': predictions_test.flatten(),\n",
    "            'Train Actual': actual_train.flatten(),\n",
    "            'Test Actual': actual_test.flatten()\n",
    "        }\n",
    "\n",
    "    print(f\"Run {i+1}/20 - Train Custom R2: {train_custom_r2:.4f} - Test Custom R2: {test_custom_r2:.4f}\")\n",
    "\n",
    "best_train_results_df = pd.DataFrame({\n",
    "    'Actual_Train': best_metrics['Train Actual'],\n",
    "    'Predicted_Train': best_metrics['Train Predictions']\n",
    "})\n",
    "\n",
    "best_test_results_df = pd.DataFrame({\n",
    "    'Actual_Test': best_metrics['Test Actual'],\n",
    "    'Predicted_Test': best_metrics['Test Predictions']\n",
    "})\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Custom R2', 'Traditional R2', 'p-value', 'MAE', 'MSE', 'RMSE'],\n",
    "    'Train': [best_metrics['Train Custom R2'], best_metrics['Train R2'], best_metrics['Train p-value'], \n",
    "              best_metrics['Train MAE'], best_metrics['Train MSE'], best_metrics['Train RMSE']],\n",
    "    'Test': [best_metrics['Test Custom R2'], best_metrics['Test R2'], best_metrics['Test p-value'],\n",
    "             best_metrics['Test MAE'], best_metrics['Test MSE'], best_metrics['Test RMSE']]\n",
    "})\n",
    "\n",
    "\n",
    "ANN_Train = best_metrics['Train Predictions']\n",
    "ANN_Test = best_metrics['Test Predictions']\n",
    "with pd.ExcelWriter('ANN.xlsx') as writer:\n",
    "    best_train_results_df.to_excel(writer, sheet_name='Train', index=False)\n",
    "    best_test_results_df.to_excel(writer, sheet_name='Test', index=False)\n",
    "    metrics_df.to_excel(writer, sheet_name='Metrics', index=False)\n",
    "\n",
    "print(f\"Best model's Test Custom R2: {best_metrics['Test Custom R2']:.4f}\")\n",
    "print(\"Predicted vs Actual values and metrics for the best model have been exported to ANN_South_Best.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea722dca-8c92-42d3-bdf0-1b44614d48e6",
   "metadata": {},
   "source": [
    "### ANN Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d539a511-4640-4337-9724-97f23f92e272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "def calculate_month_cyclic_features(date):\n",
    "    month = date.month\n",
    "    month_sin = np.sin(2 * np.pi * month / 12)\n",
    "    month_cos = np.cos(2 * np.pi * month / 12)\n",
    "    return month_sin, month_cos\n",
    "\n",
    "def prepare_forecast_input(last_lag_values, forecast_dates, i):\n",
    "        month_sin, month_cos = calculate_month_cyclic_features(forecast_dates[i])\n",
    "\n",
    "        forecast_input = np.hstack([last_lag_values, month_sin, month_cos])\n",
    "\n",
    "    return forecast_input\n",
    "\n",
    "def forecast_next_records(model, data, lag=lag, forecast_steps=1):\n",
    "        last_lag_values = data['chl_a'].values[-lag:].flatten()\n",
    "\n",
    "        last_date = data.index[-1]\n",
    "    forecast_dates = [last_date + pd.DateOffset(months=i) for i in range(1, forecast_steps + 1)]\n",
    "\n",
    "        forecast_values = []\n",
    "\n",
    "        for i in range(forecast_steps):\n",
    "                forecast_input = prepare_forecast_input(last_lag_values, forecast_dates, i)\n",
    "\n",
    "                forecast_input_tensor = torch.tensor(forecast_input, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "                model.eval()\n",
    "        with torch.no_grad():\n",
    "            forecast_value_scaled = model(forecast_input_tensor).cpu().numpy().flatten()[0]\n",
    "\n",
    "                scaler_y = StandardScaler()\n",
    "        scaler_y.fit(data[['chl_a']].values)\n",
    "        forecast_value = scaler_y.inverse_transform(np.array([[forecast_value_scaled]])).flatten()[0]\n",
    "\n",
    "                forecast_values.append(forecast_value)\n",
    "\n",
    "                last_lag_values = np.roll(last_lag_values, -1)          last_lag_values[-1] = forecast_value  \n",
    "    return forecast_values, forecast_dates\n",
    "\n",
    "ANN_forecast_values, forecast_dates = forecast_next_records(best_model, data, lag=lag, forecast_steps=forecast_steps)\n",
    "\n",
    "for date, value in zip(forecast_dates, ANN_forecast_values):\n",
    "    print(f\"Forecasted Chl-a concentration for {date.date()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b5002-11ba-4445-a588-903dffc48bf7",
   "metadata": {},
   "source": [
    "# KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc097d5c-3f5c-45d7-8b20-667eff493758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from deepkan import SplineLinearLayer\n",
    "\n",
    "class KANTimeSeries(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, num_knots=5, spline_order=3,\n",
    "                 noise_scale=0.1, base_scale=0.1, spline_scale=1.0,\n",
    "                 activation=nn.SiLU, grid_epsilon=1, grid_range=[-1, 1]):\n",
    "        super(KANTimeSeries, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            self.layers.append(SplineLinearLayer(prev_size, hidden_size, num_knots, spline_order,\n",
    "                                                 noise_scale, base_scale, spline_scale,\n",
    "                                                 activation, grid_epsilon, grid_range))\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        self.output_layer = SplineLinearLayer(prev_size, output_size, num_knots, spline_order,\n",
    "                                              noise_scale, base_scale, spline_scale,\n",
    "                                              activation, grid_epsilon, grid_range)\n",
    "\n",
    "    def forward(self, x, update_knots=False):\n",
    "        for layer in self.layers:\n",
    "            if update_knots:\n",
    "                layer._update_knots(x)\n",
    "            x = layer(x)\n",
    "\n",
    "        if update_knots:\n",
    "            self.output_layer._update_knots(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
    "        loss = 0\n",
    "        for layer in self.layers:\n",
    "            loss += layer._regularization_loss(regularize_activation, regularize_entropy)\n",
    "        loss += self.output_layer._regularization_loss(regularize_activation, regularize_entropy)\n",
    "        return loss\n",
    "\n",
    "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
    "        loss = 0\n",
    "        for layer in self.layers:\n",
    "            loss += layer._regularization_loss(regularize_activation, regularize_entropy)\n",
    "        loss += self.output_layer._regularization_loss(regularize_activation, regularize_entropy)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259613b5-5766-4b47-b218-26e342fb35b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dat_KAN = data_LSTM[['chl_a', 'month_sin', 'month_cos']].values.astype(np.float32)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_normalized = scaler.fit_transform(dat_KAN)\n",
    "\n",
    "def create_sequences(dat_KAN, window_size):\n",
    "    sequences = []\n",
    "    for i in range(len(dat_KAN) - window_size):\n",
    "        input_seq = dat_KAN[i:i+window_size]          output = dat_KAN[i+window_size, 0]          sequences.append((input_seq, output))\n",
    "    return sequences\n",
    "\n",
    "window_size = 12  sequences = create_sequences(data_normalized, window_size)\n",
    "\n",
    "inputs_torch = torch.tensor([seq[0] for seq in sequences], dtype=torch.float32)\n",
    "targets_torch = torch.tensor([seq[1] for seq in sequences], dtype=torch.float32)\n",
    "\n",
    "split_idx = int(len(inputs_torch) * 0.8)\n",
    "train_inputs_torch = inputs_torch[:split_idx]\n",
    "test_inputs_torch = inputs_torch[split_idx:]\n",
    "train_targets_torch = targets_torch[:split_idx]\n",
    "test_targets_torch = targets_torch[split_idx:]\n",
    "\n",
    "input_size = window_size * 3  hidden_sizes = [32]\n",
    "num_knots = 5\n",
    "spline_order = 3\n",
    "model_torch = KANTimeSeries(input_size, hidden_sizes, 1, num_knots, spline_order)\n",
    "\n",
    "optimizer_torch = torch.optim.Adam(model_torch.parameters(), lr=0.01)\n",
    "criterion_torch = torch.nn.MSELoss()\n",
    "\n",
    "epochs = 110\n",
    "for epoch in range(epochs):\n",
    "    model_torch.train()\n",
    "    epoch_loss = 0\n",
    "    for i in range(len(train_inputs_torch)):\n",
    "        optimizer_torch.zero_grad()\n",
    "        output = model_torch(train_inputs_torch[i:i+1].view(1, -1))          target = train_targets_torch[i].view_as(output)\n",
    "        loss = criterion_torch(output, target)\n",
    "        loss.backward()\n",
    "        optimizer_torch.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f'KAN: Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(train_inputs_torch):.6f}')\n",
    "\n",
    "model_torch.eval()\n",
    "with torch.no_grad():\n",
    "    train_pred_torch = model_torch(train_inputs_torch.view(len(train_inputs_torch), -1)).squeeze().numpy()\n",
    "    test_pred_torch = model_torch(test_inputs_torch.view(len(test_inputs_torch), -1)).squeeze().numpy()\n",
    "\n",
    "train_pred_denorm_torch = scaler.inverse_transform(np.hstack([train_pred_torch.reshape(-1, 1), np.zeros((len(train_pred_torch), 2))]))[:, 0]\n",
    "test_pred_denorm_torch = scaler.inverse_transform(np.hstack([test_pred_torch.reshape(-1, 1), np.zeros((len(test_pred_torch), 2))]))[:, 0]\n",
    "train_actual_denorm_torch = scaler.inverse_transform(np.hstack([train_targets_torch.numpy().reshape(-1, 1), np.zeros((len(train_targets_torch), 2))]))[:, 0]\n",
    "test_actual_denorm_torch = scaler.inverse_transform(np.hstack([test_targets_torch.numpy().reshape(-1, 1), np.zeros((len(test_targets_torch), 2))]))[:, 0]\n",
    "\n",
    "model_tf = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu', input_shape=(window_size * 3,)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_tf.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history_tf = model_tf.fit(train_inputs_torch.numpy().reshape(len(train_inputs_torch), -1), train_targets_torch.numpy(), epochs=epochs, verbose=1)\n",
    "\n",
    "train_pred_tf = model_tf.predict(train_inputs_torch.numpy().reshape(len(train_inputs_torch), -1)).flatten()\n",
    "test_pred_tf = model_tf.predict(test_inputs_torch.numpy().reshape(len(test_inputs_torch), -1)).flatten()\n",
    "\n",
    "train_pred_denorm_tf = scaler.inverse_transform(np.hstack([train_pred_tf.reshape(-1, 1), np.zeros((len(train_pred_tf), 2))]))[:, 0]\n",
    "test_pred_denorm_tf = scaler.inverse_transform(np.hstack([test_pred_tf.reshape(-1, 1), np.zeros((len(test_pred_tf), 2))]))[:, 0]\n",
    "\n",
    "train_actual_denorm_tf = scaler.inverse_transform(np.hstack([train_targets_torch.numpy().reshape(-1, 1), np.zeros((len(train_targets_torch), 2))]))[:, 0]\n",
    "test_actual_denorm_tf = scaler.inverse_transform(np.hstack([test_targets_torch.numpy().reshape(-1, 1), np.zeros((len(test_targets_torch), 2))]))[:, 0]\n",
    "\n",
    "train_results_df = pd.DataFrame({\n",
    "    'Actual': train_actual_denorm_torch,\n",
    "    'KAN_Predicted': train_pred_denorm_torch,\n",
    "    'MLP_Predicted': train_pred_denorm_tf\n",
    "})\n",
    "\n",
    "test_results_df = pd.DataFrame({\n",
    "    'Actual': test_actual_denorm_torch,\n",
    "    'KAN_Predicted': test_pred_denorm_torch,\n",
    "    'MLP_Predicted': test_pred_denorm_tf\n",
    "})\n",
    "\n",
    "print(\"Training Results:\")\n",
    "print(train_results_df)\n",
    "\n",
    "print(\"Test Results:\")\n",
    "print(test_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5f03df-16ee-44cc-87bb-447b721071c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def calculate_month_cyclic_features(date):\n",
    "    month = date.month\n",
    "    month_sin = np.sin(2 * np.pi * month / 12)\n",
    "    month_cos = np.cos(2 * np.pi * month / 12)\n",
    "    return month_sin, month_cos\n",
    "\n",
    "def forecast_next_steps(model, last_window, n_steps, window_size, start_date):\n",
    "    forecasts = []\n",
    "    current_window = last_window.copy()      current_date = start_date\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "                current_window_torch = torch.tensor(current_window.reshape(1, -1), dtype=torch.float32)\n",
    "        \n",
    "                with torch.no_grad():\n",
    "            next_pred = model(current_window_torch).item()  \n",
    "                forecasts.append(next_pred)\n",
    "        \n",
    "                        current_date += datetime.timedelta(days=30)          month_sin, month_cos = calculate_month_cyclic_features(current_date)\n",
    "        \n",
    "                next_input = np.array([next_pred, month_sin, month_cos])          current_window = np.vstack((current_window[1:], next_input))          \n",
    "    return forecasts\n",
    "\n",
    "last_window = data_normalized[-window_size:]  n_steps = 6\n",
    "start_date = datetime.date(2024, 9, 1) \n",
    "\n",
    "next_6_predictions_torch = forecast_next_steps(model_torch, last_window, n_steps, window_size, start_date)\n",
    "\n",
    "next_6_predictions_denorm_torch = scaler.inverse_transform(np.hstack([np.array(next_6_predictions_torch).reshape(-1, 1), np.zeros((len(next_6_predictions_torch), 2))]))[:, 0]\n",
    "\n",
    "print(\"Forecasted next 6 chl_a values using KAN model:\")\n",
    "print(next_6_predictions_denorm_torch)\n",
    "\n",
    "def forecast_next_steps_tf(model, last_window, n_steps, window_size, start_date):\n",
    "    forecasts = []\n",
    "    current_window = last_window.copy()      current_date = start_date\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "                current_window_tf = current_window.reshape(1, -1)\n",
    "        \n",
    "                next_pred = model.predict(current_window_tf).item()          \n",
    "                forecasts.append(next_pred)\n",
    "        \n",
    "                        current_date += datetime.timedelta(days=30)          month_sin, month_cos = calculate_month_cyclic_features(current_date)\n",
    "        \n",
    "                next_input = np.array([next_pred, month_sin, month_cos])          current_window = np.vstack((current_window[1:], next_input))          \n",
    "    return forecasts\n",
    "\n",
    "next_6_predictions_tf = forecast_next_steps_tf(model_tf, last_window, n_steps, window_size, start_date)\n",
    "\n",
    "next_6_predictions_denorm_tf = scaler.inverse_transform(np.hstack([np.array(next_6_predictions_tf).reshape(-1, 1), np.zeros((len(next_6_predictions_tf), 2))]))[:, 0]\n",
    "\n",
    "print(\"Forecasted next 6 chl_a values using MLP model:\")\n",
    "print(next_6_predictions_denorm_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099892be-02f5-49d0-a413-ba29baa44f2e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from kan import KAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def custom_r2(y_true, y_pred):\n",
    "    y_true_mean = np.mean(y_true)\n",
    "    y_pred_mean = np.mean(y_pred)\n",
    "    numerator = np.mean((y_true - y_true_mean) * (y_pred - y_pred_mean))\n",
    "    denominator = np.sqrt(np.mean((y_true - y_true_mean) ** 2) * np.mean((y_pred - y_pred_mean) ** 2))\n",
    "    r = numerator / denominator\n",
    "    return r ** 2\n",
    "\n",
    "def load_chla_dataset(data, lag):\n",
    "    target = data['chl_a'].values\n",
    "        input_data = data[[f'yt-{i}' for i in range(1, lag + 1)]]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "    input_data = scaler.fit_transform(input_data)\n",
    "\n",
    "        input_tensor = torch.tensor(input_data, dtype=torch.float32, device=device)\n",
    "    target_tensor = torch.tensor(target, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "\n",
    "        train_data, test_data, train_target, test_target = train_test_split(input_tensor, target_tensor, test_size=0.2, shuffle=False, random_state=42)\n",
    "\n",
    "        return {\n",
    "        'train_input': train_data,\n",
    "        'train_label': train_target,\n",
    "        'test_input': test_data,\n",
    "        'test_label': test_target\n",
    "    }\n",
    "\n",
    "\n",
    "def build_and_train_kan_model(dataset):\n",
    "    KAN_model = KAN(\n",
    "                width=[dataset['train_input'].shape[1], 64, 1],\n",
    "        grid=3,\n",
    "        k=2,\n",
    "        noise_scale=0.1,\n",
    "        noise_scale_base=0.1,\n",
    "        symbolic_enabled=False,\n",
    "        bias_trainable=False,\n",
    "        grid_eps=0.02,\n",
    "        grid_range=[-1, 1],\n",
    "        sp_trainable=True,\n",
    "        sb_trainable=True,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "        def train_mse():\n",
    "        with torch.no_grad():\n",
    "            predictions = KAN_model(dataset['train_input'])\n",
    "            return torch.nn.functional.mse_loss(predictions, dataset['train_label'])\n",
    "\n",
    "    def test_mse():\n",
    "        with torch.no_grad():\n",
    "            predictions = KAN_model(dataset['test_input'])\n",
    "            return torch.nn.functional.mse_loss(predictions, dataset['test_label'])\n",
    "\n",
    "        results = KAN_model.train(\n",
    "        dataset, opt=\"LBFGS\", device=device, metrics=(train_mse, test_mse),\n",
    "        steps=10,\n",
    "        log=1,\n",
    "        lamb=0.01,\n",
    "        lamb_l1=1,\n",
    "        lamb_entropy=2,\n",
    "        lamb_coef=0,\n",
    "        lamb_coefdiff=0,\n",
    "        update_grid=True,\n",
    "        grid_update_num=10,\n",
    "        loss_fn=torch.nn.SmoothL1Loss(),\n",
    "        lr=0.01,\n",
    "        stop_grid_update_step=50,\n",
    "        batch=-1,\n",
    "        small_mag_threshold=1e-16,\n",
    "        small_reg_factor=0.1,\n",
    "        sglr_avoid=False\n",
    "    )\n",
    "\n",
    "        with torch.no_grad():\n",
    "        y_pred_train = KAN_model(dataset['train_input']).cpu().numpy().flatten()\n",
    "        y_pred_test = KAN_model(dataset['test_input']).cpu().numpy().flatten()\n",
    "\n",
    "        train_metrics = {\n",
    "        'R2': r2_score(dataset['train_label'].cpu().numpy(), y_pred_train),\n",
    "        'MAE': mean_absolute_error(dataset['train_label'].cpu().numpy(), y_pred_train),\n",
    "        'MSE': mean_squared_error(dataset['train_label'].cpu().numpy(), y_pred_train),\n",
    "        'RMSE': np.sqrt(mean_squared_error(dataset['train_label'].cpu().numpy(), y_pred_train)),\n",
    "        'R': np.corrcoef(dataset['train_label'].cpu().numpy().flatten(), y_pred_train)[0, 1]\n",
    "    }\n",
    "\n",
    "    test_metrics = {\n",
    "        'R2': r2_score(dataset['test_label'].cpu().numpy(), y_pred_test),\n",
    "        'MAE': mean_absolute_error(dataset['test_label'].cpu().numpy(), y_pred_test),\n",
    "        'MSE': mean_squared_error(dataset['test_label'].cpu().numpy(), y_pred_test),\n",
    "        'RMSE': np.sqrt(mean_squared_error(dataset['test_label'].cpu().numpy(), y_pred_test)),\n",
    "        'R': np.corrcoef(dataset['test_label'].cpu().numpy().flatten(), y_pred_test)[0, 1]\n",
    "    }\n",
    "\n",
    "    return KAN_model, train_metrics, test_metrics, y_pred_train, y_pred_test\n",
    "\n",
    "\n",
    "metrics_test_df = pd.DataFrame(columns=['R2', 'R', 'MSE', 'RMSE', 'MAE'])\n",
    "metrics_train_df = pd.DataFrame(columns=['R2', 'R', 'MSE', 'RMSE', 'MAE'])\n",
    "\n",
    "best_y_preds_test = {}\n",
    "best_y_preds_train = {}\n",
    "\n",
    "\n",
    "lag=lag\n",
    "chla_dataset = load_chla_dataset(data, lag=lag)\n",
    "\n",
    "KAN_model, train_metrics, test_metrics, y_pred_train, y_pred_test = build_and_train_kan_model(chla_dataset)\n",
    "\n",
    "metrics_test_df = pd.concat([metrics_test_df, pd.DataFrame([test_metrics])], ignore_index=True)\n",
    "metrics_train_df = pd.concat([metrics_train_df, pd.DataFrame([train_metrics])], ignore_index=True)\n",
    "\n",
    "best_y_preds_test['Actual'] = chla_dataset['test_label'].cpu().numpy().flatten()\n",
    "best_y_preds_test['Predicted'] = y_pred_test\n",
    "\n",
    "best_y_preds_train['Actual'] = chla_dataset['train_label'].cpu().numpy().flatten()\n",
    "best_y_preds_train['Predicted'] = y_pred_train\n",
    "\n",
    "summary_df = pd.concat([\n",
    "    pd.DataFrame({'Data': 'Train', 'R2': train_metrics['R2'], 'R': train_metrics['R'], \n",
    "                  'MSE': train_metrics['MSE'], 'RMSE': train_metrics['RMSE'], 'MAE': train_metrics['MAE']}, index=[0]),\n",
    "    pd.DataFrame({'Data': 'Test', 'R2': test_metrics['R2'], 'R': test_metrics['R'], \n",
    "                  'MSE': test_metrics['MSE'], 'RMSE': test_metrics['RMSE'], 'MAE': test_metrics['MAE']}, index=[1])\n",
    "])\n",
    "\n",
    "KAN_Train = y_pred_train\n",
    "KAN_Test = y_pred_test\n",
    "with pd.ExcelWriter('KAN.xlsx') as writer:\n",
    "    metrics_test_df.to_excel(writer, sheet_name='Test Metrics', index=False)\n",
    "    metrics_train_df.to_excel(writer, sheet_name='Train Metrics', index=False)\n",
    "    pd.DataFrame(best_y_preds_test).to_excel(writer, sheet_name='Test Predictions', index=False)\n",
    "    pd.DataFrame(best_y_preds_train).to_excel(writer, sheet_name='Train Predictions', index=False)\n",
    "    summary_df.to_excel(writer, sheet_name='Metrics Summary', index=False)\n",
    "\n",
    "print(\"Process completed and results saved to 'KAN_South.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c078e9-9889-43e0-b231-9ee948008f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "def calculate_month_cyclic_features(date):\n",
    "    month = date.month\n",
    "    month_sin = np.sin(2 * np.pi * month / 12)\n",
    "    month_cos = np.cos(2 * np.pi * month / 12)\n",
    "    return month_sin, month_cos\n",
    "\n",
    "def prepare_forecast_input(data, last_lag_values, forecast_dates, i):\n",
    "        \n",
    "            forecast_input = np.hstack([last_lag_values])\n",
    "\n",
    "        return forecast_input\n",
    "\n",
    "def forecast_next_records(KAN_model, data, lag=lag, forecast_steps=forecast_steps, device='cpu'):\n",
    "        last_lag_values = data['chl_a'].values[-lag:].flatten()\n",
    "\n",
    "        last_date = data.index[-1]\n",
    "    forecast_dates = [last_date + pd.DateOffset(months=i) for i in range(1, forecast_steps + 1)]\n",
    "\n",
    "        forecast_values = []\n",
    "    for i in range(forecast_steps):\n",
    "                forecast_input = prepare_forecast_input(data, last_lag_values, forecast_dates, i)\n",
    "        \n",
    "                forecast_input_tensor = torch.tensor(forecast_input, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        print(forecast_input_tensor)\n",
    "                KAN_model.to(device)\n",
    "        \n",
    "                with torch.no_grad():\n",
    "            forecast_value_scaled = KAN_model(forecast_input_tensor).cpu().numpy().flatten()[0]\n",
    "        \n",
    "                scaler_y = StandardScaler()\n",
    "        scaler_y.fit(data[['chl_a']].values)\n",
    "        forecast_value = scaler_y.inverse_transform(np.array([[forecast_value_scaled]])).flatten()[0]\n",
    "        \n",
    "                forecast_values.append(forecast_value)\n",
    "\n",
    "                last_lag_values = np.roll(last_lag_values, -1)          last_lag_values[-1] = forecast_value          \n",
    "    \n",
    "    return forecast_values, forecast_dates\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "KAN_forecast_values, forecast_dates = forecast_next_records(KAN_model, data, lag=lag, forecast_steps=forecast_steps, device=device)\n",
    "\n",
    "for date, value in zip(forecast_dates, KAN_forecast_values):\n",
    "    print(f\"Forecasted Chl-a concentration for {date.date()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e91aab0-cbbb-4dfc-a1db-f9659e3398c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "forecast_data = {\n",
    "    'Date': forecast_dates,      'Actual': data_for_forecasting['chl_a'].values,      'ANN_Forecast': ANN_forecast_values,\n",
    "    'KAN_Forecast': KAN_forecast_values,\n",
    "    'RF_Forecast': RF_forecast_values,\n",
    "    'GPR_Forecast': GPR_forecast_values,\n",
    "    'SVR_Forecast': SVR_forecast_values\n",
    "}\n",
    "\n",
    "forecast_df = pd.DataFrame(forecast_data)\n",
    "\n",
    "forecast_df.to_excel('forecast.xlsx', index=False)\n",
    "\n",
    "print(\"Forecast results have been successfully exported to 'forecast.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a6089c-3f5f-4613-95b3-f001ebd6e3b8",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b508abfa-a7ee-4e89-b758-2bcf8ab6518f",
   "metadata": {},
   "source": [
    "### Hyperparameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715f27b4-fb4c-496a-92c4-c3bbf956f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout_rate):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]          out = self.fc(lstm_out)\n",
    "        return out\n",
    "\n",
    "def prepare_data_with_time_steps(data_LSTM, time_steps=lag):\n",
    "        chl_a_values = data_LSTM['chl_a'].values\n",
    "    month_sin = data_LSTM['month_sin'].values\n",
    "    month_cos = data_LSTM['month_cos'].values\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(chl_a_values) - time_steps):\n",
    "                X.append(\n",
    "               np.hstack([chl_a_values[i:i + time_steps].reshape(-1, 1), \n",
    "               np.tile([month_sin[i + time_steps], month_cos[i + time_steps]], (time_steps, 1))])\n",
    "        )\n",
    "\n",
    "        y.append(chl_a_values[i + time_steps])  \n",
    "    X, y = np.array(X), np.array(y).reshape(-1, 1)\n",
    "    \n",
    "        scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "        X = X.reshape(X.shape[0], -1)      X = scaler_X.fit_transform(X)      X = X.reshape(X.shape[0], time_steps, -1)  \n",
    "        y = scaler_y.fit_transform(y)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "    \n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32, device=device)\n",
    "\n",
    "    return X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, scaler_y\n",
    "\n",
    "def train_and_evaluate_lstm(X_train, y_train, X_test, y_test, input_size, hidden_size, output_size, num_layers, dropout_rate, lr, num_epochs=100):\n",
    "    lstm_model = LSTMModel(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers, dropout_rate=dropout_rate).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(lstm_model.parameters(), lr=lr)\n",
    "    \n",
    "        for epoch in range(num_epochs):\n",
    "        lstm_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = lstm_model(X_train)\n",
    "        loss = criterion(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_test_pred = lstm_model(X_test).cpu().numpy()\n",
    "    \n",
    "        r2 = r2_score(y_test.cpu().numpy(), y_test_pred)\n",
    "    return r2, lstm_model\n",
    "\n",
    "\n",
    "time_steps = lag\n",
    "input_size = 3  output_size = 1\n",
    "\n",
    "X_train, y_train, X_test, y_test, scaler_y = prepare_data_with_time_steps(data_LSTM, time_steps=time_steps)\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'num_layers': [1, 2, 3],\n",
    "    'hidden_size': [16, 32, 64],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "best_r2 = -float('inf')\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"Testing parameters: {params}\")\n",
    "    r2, lstm_model = train_and_evaluate_lstm(\n",
    "        X_train, y_train, X_test, y_test, \n",
    "        input_size=input_size, \n",
    "        hidden_size=params['hidden_size'], \n",
    "        output_size=output_size,\n",
    "        num_layers=params['num_layers'], \n",
    "        dropout_rate=params['dropout_rate'], \n",
    "        lr=params['learning_rate'],\n",
    "        num_epochs=100      )\n",
    "\n",
    "    if r2 > best_r2:\n",
    "        best_r2 = r2\n",
    "        best_params = params\n",
    "        best_model = lstm_model\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best R2 score: {best_r2}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16c117f-eefc-460c-bed0-48c2cf4f6336",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb70cc2a-571d-4741-b3d2-6476a8fb139f",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323a3b95-ad23-45e1-8448-fe102baebc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def custom_r2(y_true, y_pred):\n",
    "    y_true_mean = np.mean(y_true)\n",
    "    y_pred_mean = np.mean(y_pred)\n",
    "    numerator = np.mean((y_true - y_true_mean) * (y_pred - y_pred_mean))\n",
    "    denominator = np.sqrt(np.mean((y_true - y_true_mean) ** 2) * np.mean((y_pred - y_pred_mean) ** 2))\n",
    "    r = numerator / denominator\n",
    "    return r ** 2\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, dropout=0)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]          out = self.fc(lstm_out)\n",
    "        return out\n",
    "\n",
    "def prepare_data_with_time_steps(data_LSTM, time_steps=lag):\n",
    "        chl_a_values = data_LSTM['chl_a'].values\n",
    "    month_sin = data_LSTM['month_sin'].values\n",
    "    month_cos = data_LSTM['month_cos'].values\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(chl_a_values) - time_steps):\n",
    "                X.append(\n",
    "               np.hstack([chl_a_values[i:i + time_steps].reshape(-1, 1), \n",
    "               np.tile([month_sin[i + time_steps], month_cos[i + time_steps]], (time_steps, 1))])\n",
    "        )\n",
    "\n",
    "        y.append(chl_a_values[i + time_steps])  \n",
    "    X, y = np.array(X), np.array(y).reshape(-1, 1)\n",
    "    \n",
    "        scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "        X = X.reshape(X.shape[0], -1)      X = scaler_X.fit_transform(X)      X = X.reshape(X.shape[0], time_steps, -1)  \n",
    "        y = scaler_y.fit_transform(y)\n",
    "\n",
    "    \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "    \n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32, device=device)\n",
    "\n",
    "    return X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, scaler_y\n",
    "\n",
    "def train_and_evaluate_lstm(X_train, y_train, X_test, y_test, input_size, hidden_size, output_size, num_epochs=100, lr=0.01):\n",
    "    lstm_model = LSTMModel(input_size=input_size, hidden_size=hidden_size, output_size=output_size).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(lstm_model.parameters(), lr=lr)\n",
    "    \n",
    "        for epoch in range(num_epochs):\n",
    "        lstm_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = lstm_model(X_train)\n",
    "        loss = criterion(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train_pred = lstm_model(X_train).cpu().numpy()\n",
    "        y_test_pred = lstm_model(X_test).cpu().numpy()\n",
    "\n",
    "        train_metrics = {\n",
    "        'R2': r2_score(y_train.cpu().numpy(), y_train_pred),\n",
    "        'MAE': mean_absolute_error(y_train.cpu().numpy(), y_train_pred),\n",
    "        'MSE': mean_squared_error(y_train.cpu().numpy(), y_train_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_train.cpu().numpy(), y_train_pred)),\n",
    "        'R': np.corrcoef(y_train.cpu().numpy().flatten(), y_train_pred.flatten())[0, 1]\n",
    "    }\n",
    "\n",
    "    test_metrics = {\n",
    "        'R2': r2_score(y_test.cpu().numpy(), y_test_pred),\n",
    "        'MAE': mean_absolute_error(y_test.cpu().numpy(), y_test_pred),\n",
    "        'MSE': mean_squared_error(y_test.cpu().numpy(), y_test_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test.cpu().numpy(), y_test_pred)),\n",
    "        'R': np.corrcoef(y_test.cpu().numpy().flatten(), y_test_pred.flatten())[0, 1]\n",
    "    }\n",
    "\n",
    "        return train_metrics, test_metrics, y_train_pred, y_test_pred, lstm_model\n",
    "\n",
    "\n",
    "best_r2 = -float('inf')\n",
    "best_train_metrics = None\n",
    "best_test_metrics = None\n",
    "best_y_train_pred = None\n",
    "best_y_test_pred = None\n",
    "best_lstm_model = None  \n",
    "time_steps = lag\n",
    "input_size = 3  hidden_size = 32\n",
    "output_size = 1\n",
    "\n",
    "X_train, y_train, X_test, y_test, scaler_y = prepare_data_with_time_steps(data_LSTM, time_steps=time_steps)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Run {i+1}/20\")\n",
    "    train_metrics, test_metrics, y_train_pred, y_test_pred, lstm_model = train_and_evaluate_lstm(\n",
    "        X_train, y_train, X_test, y_test, input_size, hidden_size, output_size, num_epochs=100, lr=0.01\n",
    "    )\n",
    "\n",
    "        if test_metrics['R2'] > best_r2:\n",
    "        best_r2 = test_metrics['R2']\n",
    "        best_train_metrics = train_metrics\n",
    "        best_test_metrics = test_metrics\n",
    "        best_y_train_pred = y_train_pred\n",
    "        best_y_test_pred = y_test_pred\n",
    "        best_lstm_model = lstm_model  \n",
    "\n",
    "\n",
    "best_y_train_pred = scaler_y.inverse_transform(best_y_train_pred)\n",
    "best_y_test_pred = scaler_y.inverse_transform(best_y_test_pred)\n",
    "y_train_actual = scaler_y.inverse_transform(y_train.cpu().numpy())\n",
    "y_test_actual = scaler_y.inverse_transform(y_test.cpu().numpy())\n",
    "\n",
    "metrics_test_df = pd.DataFrame(columns=['R2', 'R', 'MSE', 'RMSE', 'MAE'])\n",
    "metrics_train_df = pd.DataFrame(columns=['R2', 'R', 'MSE', 'RMSE', 'MAE'])\n",
    "\n",
    "best_y_preds_test = {'Actual': y_test_actual.flatten(), 'Predicted': best_y_test_pred.flatten()}\n",
    "best_y_preds_train = {'Actual': y_train_actual.flatten(), 'Predicted': best_y_train_pred.flatten()}\n",
    "\n",
    "metrics_test_df = pd.concat([metrics_test_df, pd.DataFrame([best_test_metrics])], ignore_index=True)\n",
    "metrics_train_df = pd.concat([metrics_train_df, pd.DataFrame([best_train_metrics])], ignore_index=True)\n",
    "\n",
    "summary_df = pd.concat([\n",
    "    pd.DataFrame({'Data': 'Train', 'R2': best_train_metrics['R2'], 'R': best_train_metrics['R'], \n",
    "                  'MSE': best_train_metrics['MSE'], 'RMSE': best_train_metrics['RMSE'], 'MAE': best_train_metrics['MAE']}, index=[0]),\n",
    "    pd.DataFrame({'Data': 'Test', 'R2': best_test_metrics['R2'], 'R': best_test_metrics['R'], \n",
    "                  'MSE': best_test_metrics['MSE'], 'RMSE': best_test_metrics['RMSE'], 'MAE': best_test_metrics['MAE']}, index=[1])\n",
    "])\n",
    "\n",
    "LSTM_Train = best_y_train_pred.flatten()\n",
    "LSTM_Test = best_y_test_pred.flatten()\n",
    "with pd.ExcelWriter('LSTM.xlsx') as writer:\n",
    "    metrics_test_df.to_excel(writer, sheet_name='Test Metrics', index=False)\n",
    "    metrics_train_df.to_excel(writer, sheet_name='Train Metrics', index=False)\n",
    "    pd.DataFrame(best_y_preds_test).to_excel(writer, sheet_name='Test Predictions', index=False)\n",
    "    pd.DataFrame(best_y_preds_train).to_excel(writer, sheet_name='Train Predictions', index=False)\n",
    "    summary_df.to_excel(writer, sheet_name='Metrics Summary', index=False)\n",
    "\n",
    "print(\"Process completed and results saved to 'LSTM_South_Best_R2.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9382a94-2e38-47f4-9903-e23dffbc0bdc",
   "metadata": {},
   "source": [
    "### LSTM Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cd9619-36a2-427b-89e8-4c9d63453363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def calculate_month_cyclic_features(date):\n",
    "    month = date.month\n",
    "    month_sin = np.sin(2 * np.pi * month / 12)\n",
    "    month_cos = np.cos(2 * np.pi * month / 12)\n",
    "    return month_sin, month_cos\n",
    "\n",
    "def prepare_forecast_input_lstm(last_lag_values, forecast_dates, i, lag=lag, device='cpu'):\n",
    "        month_sin, month_cos = calculate_month_cyclic_features(forecast_dates[i])\n",
    "\n",
    "        forecast_input = np.hstack([last_lag_values.reshape(-1, 1), np.tile([month_sin, month_cos], (lag, 1))])\n",
    "    \n",
    "        forecast_input_tensor = torch.tensor(forecast_input, dtype=torch.float32, device=device).unsqueeze(0)  \n",
    "    return forecast_input_tensor\n",
    "\n",
    "def forecast_next_records_lstm(lstm_model, data, lag=lag, forecast_steps=1, device='cpu'):\n",
    "        last_lag_values = data['chl_a'].values[-lag:].flatten()\n",
    "\n",
    "        last_date = data.index[-1]\n",
    "    forecast_dates = [last_date + pd.DateOffset(months=i) for i in range(1, forecast_steps + 1)]\n",
    "\n",
    "        forecast_values = []\n",
    "\n",
    "        for i in range(forecast_steps):\n",
    "                forecast_input_tensor = prepare_forecast_input_lstm(last_lag_values, forecast_dates, i, lag=lag, device=device)\n",
    "\n",
    "                lstm_model.eval()\n",
    "        with torch.no_grad():\n",
    "            forecast_value_scaled = lstm_model(forecast_input_tensor).cpu().numpy().flatten()[0]\n",
    "\n",
    "                scaler_y = StandardScaler()\n",
    "        scaler_y.fit(data[['chl_a']].values)\n",
    "        forecast_value = scaler_y.inverse_transform(np.array([[forecast_value_scaled]])).flatten()[0]\n",
    "\n",
    "                forecast_values.append(forecast_value)\n",
    "\n",
    "                last_lag_values = np.roll(last_lag_values, -1)          last_lag_values[-1] = forecast_value  \n",
    "    return forecast_values, forecast_dates\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "LSTM_forecast_values, forecast_dates = forecast_next_records_lstm(best_lstm_model, data_LSTM, lag=lag, forecast_steps=forecast_steps, device=device)\n",
    "\n",
    "for date, value in zip(forecast_dates, LSTM_forecast_values):\n",
    "    print(f\"Forecasted Chl-a concentration for {date.date()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0907c6c-c83c-4246-8cd7-820a17cca374",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36878188-1a7b-41db-8d7a-798267579b56",
   "metadata": {},
   "source": [
    "### Hyperparameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a8e257-9b49-44f7-ad84-dd9bf4eacde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import r2_score\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout_rate):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        gru_out = gru_out[:, -1, :]          out = self.fc(gru_out)\n",
    "        return out\n",
    "\n",
    "def train_and_evaluate_gru(X_train, y_train, X_test, y_test, input_size, hidden_size, output_size, num_layers, dropout_rate, lr, num_epochs=100):\n",
    "    gru_model = GRUModel(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers, dropout_rate=dropout_rate).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(gru_model.parameters(), lr=lr)\n",
    "    \n",
    "        for epoch in range(num_epochs):\n",
    "        gru_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = gru_model(X_train)\n",
    "        loss = criterion(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        gru_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_test_pred = gru_model(X_test).cpu().numpy()\n",
    "\n",
    "        r2 = r2_score(y_test.cpu().numpy(), y_test_pred)\n",
    "    return r2, gru_model\n",
    "\n",
    "time_steps = lag\n",
    "input_size = 3  output_size = 1\n",
    "\n",
    "X_train, y_train, X_test, y_test, scaler_y = prepare_data_with_time_steps(data_LSTM, time_steps=time_steps)\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'num_layers': [1, 2, 3],\n",
    "    'hidden_size': [16, 32, 64],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "best_r2 = -float('inf')\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"Testing parameters: {params}\")\n",
    "    r2, gru_model = train_and_evaluate_gru(\n",
    "        X_train, y_train, X_test, y_test, \n",
    "        input_size=input_size, \n",
    "        hidden_size=params['hidden_size'], \n",
    "        output_size=output_size,\n",
    "        num_layers=params['num_layers'], \n",
    "        dropout_rate=params['dropout_rate'], \n",
    "        lr=params['learning_rate'],\n",
    "        num_epochs=100      )\n",
    "\n",
    "        if r2 > best_r2:\n",
    "        best_r2 = r2\n",
    "        best_params = params\n",
    "        best_model = gru_model\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best R2 score: {best_r2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cde6659-a55b-48a7-b540-51825b548a21",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07407a8b-fcc3-4dd3-8e42-7fc659275fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def custom_r2(y_true, y_pred):\n",
    "    y_true_mean = np.mean(y_true)\n",
    "    y_pred_mean = np.mean(y_pred)\n",
    "    numerator = np.mean((y_true - y_true_mean) * (y_pred - y_pred_mean))\n",
    "    denominator = np.sqrt(np.mean((y_true - y_true_mean) ** 2) * np.mean((y_pred - y_pred_mean) ** 2))\n",
    "    r = numerator / denominator\n",
    "    return r ** 2\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        gru_out = gru_out[:, -1, :]          out = self.fc(gru_out)\n",
    "        return out\n",
    "\n",
    "def prepare_data_with_time_steps(data_lstm, time_steps=lag):\n",
    "    chl_a_values = data_lstm['chl_a'].values\n",
    "    month_sin = data_lstm['month_sin'].values\n",
    "    month_cos = data_lstm['month_cos'].values\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(chl_a_values) - time_steps):\n",
    "        X.append(\n",
    "               np.hstack([chl_a_values[i:i + time_steps].reshape(-1, 1), \n",
    "               np.tile([month_sin[i + time_steps], month_cos[i + time_steps]], (time_steps, 1))])\n",
    "        )\n",
    "        y.append(chl_a_values[i + time_steps])\n",
    "\n",
    "    X, y = np.array(X), np.array(y).reshape(-1, 1)\n",
    "    \n",
    "        scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    X = X.reshape(X.shape[0], -1)      X = scaler_X.fit_transform(X)\n",
    "    X = X.reshape(X.shape[0], time_steps, -1)  \n",
    "        y = scaler_y.fit_transform(y)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32, device=device)\n",
    "\n",
    "    return X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, scaler_y\n",
    "\n",
    "def train_and_evaluate_gru(X_train, y_train, X_test, y_test, input_size, hidden_size, output_size, num_epochs=100, lr=0.001):\n",
    "    gru_model = GRUModel(input_size=input_size, hidden_size=hidden_size, output_size=output_size).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(gru_model.parameters(), lr=lr)\n",
    "    \n",
    "        for epoch in range(num_epochs):\n",
    "        gru_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = gru_model(X_train)\n",
    "        loss = criterion(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        gru_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train_pred = gru_model(X_train).cpu().numpy()\n",
    "        y_test_pred = gru_model(X_test).cpu().numpy()\n",
    "\n",
    "        train_metrics = {\n",
    "        'R2': r2_score(y_train.cpu().numpy(), y_train_pred),\n",
    "        'MAE': mean_absolute_error(y_train.cpu().numpy(), y_train_pred),\n",
    "        'MSE': mean_squared_error(y_train.cpu().numpy(), y_train_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_train.cpu().numpy(), y_train_pred)),\n",
    "        'R': np.corrcoef(y_train.cpu().numpy().flatten(), y_train_pred.flatten())[0, 1]\n",
    "    }\n",
    "\n",
    "    test_metrics = {\n",
    "        'R2': r2_score(y_test.cpu().numpy(), y_test_pred),\n",
    "        'MAE': mean_absolute_error(y_test.cpu().numpy(), y_test_pred),\n",
    "        'MSE': mean_squared_error(y_test.cpu().numpy(), y_test_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test.cpu().numpy(), y_test_pred)),\n",
    "        'R': np.corrcoef(y_test.cpu().numpy().flatten(), y_test_pred.flatten())[0, 1]\n",
    "    }\n",
    "\n",
    "        return train_metrics, test_metrics, y_train_pred, y_test_pred, gru_model\n",
    "\n",
    "\n",
    "best_r2 = -float('inf')\n",
    "best_train_metrics = None\n",
    "best_test_metrics = None\n",
    "best_y_train_pred = None\n",
    "best_y_test_pred = None\n",
    "best_gru_model = None  \n",
    "time_steps = lag\n",
    "input_size = 3  hidden_size = 32\n",
    "output_size = 1\n",
    "\n",
    "X_train, y_train, X_test, y_test, scaler_y = prepare_data_with_time_steps(data_LSTM, time_steps=time_steps)\n",
    "\n",
    "for i in range(20):\n",
    "    print(f\"Run {i+1}/20\")\n",
    "    train_metrics, test_metrics, y_train_pred, y_test_pred, gru_model = train_and_evaluate_gru(\n",
    "        X_train, y_train, X_test, y_test, input_size, hidden_size, output_size, num_epochs=500, lr=0.001\n",
    "    )\n",
    "\n",
    "        if test_metrics['R2'] > best_r2:\n",
    "        best_r2 = test_metrics['R2']\n",
    "        best_train_metrics = train_metrics\n",
    "        best_test_metrics = test_metrics\n",
    "        best_y_train_pred = y_train_pred\n",
    "        best_y_test_pred = y_test_pred\n",
    "        best_gru_model = gru_model  \n",
    "\n",
    "best_y_train_pred = scaler_y.inverse_transform(best_y_train_pred)\n",
    "best_y_test_pred = scaler_y.inverse_transform(best_y_test_pred)\n",
    "y_train_actual = scaler_y.inverse_transform(y_train.cpu().numpy())\n",
    "y_test_actual = scaler_y.inverse_transform(y_test.cpu().numpy())\n",
    "\n",
    "metrics_test_df = pd.DataFrame(columns=['R2', 'R', 'MSE', 'RMSE', 'MAE'])\n",
    "metrics_train_df = pd.DataFrame(columns=['R2', 'R', 'MSE', 'RMSE', 'MAE'])\n",
    "\n",
    "best_y_preds_test = {'Actual': y_test_actual.flatten(), 'Predicted': best_y_test_pred.flatten()}\n",
    "best_y_preds_train = {'Actual': y_train_actual.flatten(), 'Predicted': best_y_train_pred.flatten()}\n",
    "\n",
    "metrics_test_df = pd.concat([metrics_test_df, pd.DataFrame([best_test_metrics])], ignore_index=True)\n",
    "metrics_train_df = pd.concat([metrics_train_df, pd.DataFrame([best_train_metrics])], ignore_index=True)\n",
    "\n",
    "summary_df = pd.concat([\n",
    "    pd.DataFrame({'Data': 'Train', 'R2': best_train_metrics['R2'], 'R': best_train_metrics['R'], \n",
    "                  'MSE': best_train_metrics['MSE'], 'RMSE': best_train_metrics['RMSE'], 'MAE': best_train_metrics['MAE']}, index=[0]),\n",
    "    pd.DataFrame({'Data': 'Test', 'R2': best_test_metrics['R2'], 'R': best_test_metrics['R'], \n",
    "                  'MSE': best_test_metrics['MSE'], 'RMSE': best_test_metrics['RMSE'], 'MAE': best_test_metrics['MAE']}, index=[1])\n",
    "])\n",
    "\n",
    "with pd.ExcelWriter('GRU.xlsx') as writer:\n",
    "    metrics_test_df.to_excel(writer, sheet_name='Test Metrics', index=False)\n",
    "    metrics_train_df.to_excel(writer, sheet_name='Train Metrics', index=False)\n",
    "    pd.DataFrame(best_y_preds_test).to_excel(writer, sheet_name='Test Predictions', index=False)\n",
    "    pd.DataFrame(best_y_preds_train).to_excel(writer, sheet_name='Train Predictions', index=False)\n",
    "    summary_df.to_excel(writer, sheet_name='Metrics Summary', index=False)\n",
    "\n",
    "print(\"Process completed and results saved to 'Best_GRU_South_TimeSteps.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d94e46-9e8f-4dcc-b437-a2493d5eaab6",
   "metadata": {},
   "source": [
    "### GRU Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b23d1e7-234c-4a95-8fa9-812d57ef56b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def calculate_month_cyclic_features(date):\n",
    "    month = date.month\n",
    "    month_sin = np.sin(2 * np.pi * month / 12)\n",
    "    month_cos = np.cos(2 * np.pi * month / 12)\n",
    "    return month_sin, month_cos\n",
    "\n",
    "def prepare_forecast_input_gru(last_lag_values, forecast_dates, i, lag=lag, device='cpu'):\n",
    "        month_sin, month_cos = calculate_month_cyclic_features(forecast_dates[i])\n",
    "\n",
    "        forecast_input = np.hstack([last_lag_values.reshape(-1, 1), np.tile([month_sin, month_cos], (lag, 1))])\n",
    "    \n",
    "        forecast_input_tensor = torch.tensor(forecast_input, dtype=torch.float32, device=device).unsqueeze(0)  \n",
    "    return forecast_input_tensor\n",
    "\n",
    "def forecast_next_records_gru(gru_model, data, lag=lag, forecast_steps=1, device='cpu'):\n",
    "        last_lag_values = data['chl_a'].values[-lag:].flatten()\n",
    "\n",
    "        last_date = data.index[-1]\n",
    "    forecast_dates = [last_date + pd.DateOffset(months=i) for i in range(1, forecast_steps + 1)]\n",
    "\n",
    "        forecast_values = []\n",
    "\n",
    "        for i in range(forecast_steps):\n",
    "                forecast_input_tensor = prepare_forecast_input_gru(last_lag_values, forecast_dates, i, lag=lag, device=device)\n",
    "\n",
    "                gru_model.eval()\n",
    "        with torch.no_grad():\n",
    "            forecast_value_scaled = gru_model(forecast_input_tensor).cpu().numpy().flatten()[0]\n",
    "\n",
    "                scaler_y = StandardScaler()\n",
    "        scaler_y.fit(data[['chl_a']].values)\n",
    "        forecast_value = scaler_y.inverse_transform(np.array([[forecast_value_scaled]])).flatten()[0]\n",
    "\n",
    "                forecast_values.append(forecast_value)\n",
    "\n",
    "                last_lag_values = np.roll(last_lag_values, -1)          last_lag_values[-1] = forecast_value  \n",
    "    return forecast_values, forecast_dates\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "GRU_forecast_values, forecast_dates = forecast_next_records_gru(best_gru_model, data_LSTM, lag=lag, forecast_steps=forecast_steps, device=device)\n",
    "\n",
    "for date, value in zip(forecast_dates, GRU_forecast_values):\n",
    "    print(f\"Forecasted Chl-a concentration for {date.date()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfac4178-e2f0-4a33-8a66-0bfadaee29eb",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac78ae6c-b536-43f9-84c3-4fca41392d7b",
   "metadata": {},
   "source": [
    "### Hyperparameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa58e0c7-c33a-44ea-a7af-44f7bc409e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def custom_r2(y_true, y_pred):\n",
    "    y_true_mean = np.mean(y_true)\n",
    "    y_pred_mean = np.mean(y_pred)\n",
    "    numerator = np.mean((y_true - y_true_mean) * (y_pred - y_pred_mean))\n",
    "    denominator = np.sqrt(np.mean((y_true - y_true_mean) ** 2) * np.mean((y_pred - y_pred_mean) ** 2))\n",
    "    r = numerator / denominator\n",
    "    return r ** 2\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        gru_out = gru_out[:, -1, :]          out = self.fc(gru_out)\n",
    "        return out\n",
    "\n",
    "def prepare_data_with_time_steps(data_LSTM, time_steps=lag):\n",
    "    chl_a_values = data_LSTM['chl_a'].values\n",
    "    month_sin = data_LSTM['month_sin'].values\n",
    "    month_cos = data_LSTM['month_cos'].values\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(chl_a_values) - time_steps):\n",
    "        X.append(\n",
    "               np.hstack([chl_a_values[i:i + time_steps].reshape(-1, 1), \n",
    "               np.tile([month_sin[i + time_steps], month_cos[i + time_steps]], (time_steps, 1))])\n",
    "        )\n",
    "        y.append(chl_a_values[i + time_steps])\n",
    "\n",
    "    X, y = np.array(X), np.array(y).reshape(-1, 1)\n",
    "    \n",
    "        scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    X = X.reshape(X.shape[0], -1)      X = scaler_X.fit_transform(X)\n",
    "    X = X.reshape(X.shape[0], time_steps, -1)  \n",
    "        y = scaler_y.fit_transform(y)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32, device=device)\n",
    "\n",
    "    return X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, scaler_y\n",
    "\n",
    "def train_and_evaluate_gru(X_train, y_train, X_test, y_test, input_size, hidden_size, output_size, num_epochs=100, lr=0.01):\n",
    "    gru_model = GRUModel(input_size=input_size, hidden_size=hidden_size, output_size=output_size).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(gru_model.parameters(), lr=lr)\n",
    "    \n",
    "        for epoch in range(num_epochs):\n",
    "        gru_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = gru_model(X_train)\n",
    "        loss = criterion(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        gru_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train_pred = gru_model(X_train).cpu().numpy()\n",
    "        y_test_pred = gru_model(X_test).cpu().numpy()\n",
    "\n",
    "        train_metrics = {\n",
    "        'R2': r2_score(y_train.cpu().numpy(), y_train_pred),\n",
    "        'MAE': mean_absolute_error(y_train.cpu().numpy(), y_train_pred),\n",
    "        'MSE': mean_squared_error(y_train.cpu().numpy(), y_train_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_train.cpu().numpy(), y_train_pred)),\n",
    "        'R': np.corrcoef(y_train.cpu().numpy().flatten(), y_train_pred.flatten())[0, 1]\n",
    "    }\n",
    "\n",
    "    test_metrics = {\n",
    "        'R2': r2_score(y_test.cpu().numpy(), y_test_pred),\n",
    "        'MAE': mean_absolute_error(y_test.cpu().numpy(), y_test_pred),\n",
    "        'MSE': mean_squared_error(y_test.cpu().numpy(), y_test_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test.cpu().numpy(), y_test_pred)),\n",
    "        'R': np.corrcoef(y_test.cpu().numpy().flatten(), y_test_pred.flatten())[0, 1]\n",
    "    }\n",
    "\n",
    "        return train_metrics, test_metrics, y_train_pred, y_test_pred, gru_model\n",
    "\n",
    "\n",
    "best_r2 = -float('inf')\n",
    "best_train_metrics = None\n",
    "best_test_metrics = None\n",
    "best_y_train_pred = None\n",
    "best_y_test_pred = None\n",
    "best_gru_model = None  \n",
    "time_steps = lag\n",
    "input_size = 3  hidden_size = 32\n",
    "output_size = 1\n",
    "\n",
    "X_train, y_train, X_test, y_test, scaler_y = prepare_data_with_time_steps(data_LSTM, time_steps=time_steps)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Run {i+1}/20\")\n",
    "    train_metrics, test_metrics, y_train_pred, y_test_pred, gru_model = train_and_evaluate_gru(\n",
    "        X_train, y_train, X_test, y_test, input_size, hidden_size, output_size, num_epochs=100, lr=0.01\n",
    "    )\n",
    "\n",
    "        if test_metrics['R2'] > best_r2:\n",
    "        best_r2 = test_metrics['R2']\n",
    "        best_train_metrics = train_metrics\n",
    "        best_test_metrics = test_metrics\n",
    "        best_y_train_pred = y_train_pred\n",
    "        best_y_test_pred = y_test_pred\n",
    "        best_gru_model = gru_model  \n",
    "\n",
    "best_y_train_pred = scaler_y.inverse_transform(best_y_train_pred)\n",
    "best_y_test_pred = scaler_y.inverse_transform(best_y_test_pred)\n",
    "y_train_actual = scaler_y.inverse_transform(y_train.cpu().numpy())\n",
    "y_test_actual = scaler_y.inverse_transform(y_test.cpu().numpy())\n",
    "\n",
    "metrics_test_df = pd.DataFrame(columns=['R2', 'R', 'MSE', 'RMSE', 'MAE'])\n",
    "metrics_train_df = pd.DataFrame(columns=['R2', 'R', 'MSE', 'RMSE', 'MAE'])\n",
    "\n",
    "best_y_preds_test = {'Actual': y_test_actual.flatten(), 'Predicted': best_y_test_pred.flatten()}\n",
    "best_y_preds_train = {'Actual': y_train_actual.flatten(), 'Predicted': best_y_train_pred.flatten()}\n",
    "\n",
    "metrics_test_df = pd.concat([metrics_test_df, pd.DataFrame([best_test_metrics])], ignore_index=True)\n",
    "metrics_train_df = pd.concat([metrics_train_df, pd.DataFrame([best_train_metrics])], ignore_index=True)\n",
    "\n",
    "summary_df = pd.concat([\n",
    "    pd.DataFrame({'Data': 'Train', 'R2': best_train_metrics['R2'], 'R': best_train_metrics['R'], \n",
    "                  'MSE': best_train_metrics['MSE'], 'RMSE': best_train_metrics['RMSE'], 'MAE': best_train_metrics['MAE']}, index=[0]),\n",
    "    pd.DataFrame({'Data': 'Test', 'R2': best_test_metrics['R2'], 'R': best_test_metrics['R'], \n",
    "                  'MSE': best_test_metrics['MSE'], 'RMSE': best_test_metrics['RMSE'], 'MAE': best_test_metrics['MAE']}, index=[1])\n",
    "])\n",
    "GRU_Train = best_y_train_pred.flatten()\n",
    "GRU_Test = best_y_test_pred.flatten()\n",
    "\n",
    "with pd.ExcelWriter('GRU.xlsx') as writer:\n",
    "    metrics_test_df.to_excel(writer, sheet_name='Test Metrics', index=False)\n",
    "    metrics_train_df.to_excel(writer, sheet_name='Train Metrics', index=False)\n",
    "    pd.DataFrame(best_y_preds_test).to_excel(writer, sheet_name='Test Predictions', index=False)\n",
    "    pd.DataFrame(best_y_preds_train).to_excel(writer, sheet_name='Train Predictions', index=False)\n",
    "    summary_df.to_excel(writer, sheet_name='Metrics Summary', index=False)\n",
    "\n",
    "print(\"Process completed and results saved to 'Best_GRU_South_TimeSteps.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1019b8e4-7f19-4157-9966-9f744b58edee",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e23140d-8cbd-467a-8e8a-df0ce64750fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate_rf(X_train, y_train, X_test, y_test, n_estimators=50, max_depth=None):\n",
    "    rf_model = RandomForestRegressor(n_estimators=n_estimators, max_depth=3, random_state=42)\n",
    "\n",
    "        rf_model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = rf_model.predict(X_train)\n",
    "    y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "        train_metrics = {\n",
    "        'R2': r2_score(y_train, y_train_pred),\n",
    "        'MAE': mean_absolute_error(y_train, y_train_pred),\n",
    "        'MSE': mean_squared_error(y_train, y_train_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "        'R': np.corrcoef(y_train, y_train_pred)[0, 1]\n",
    "    }\n",
    "\n",
    "    test_metrics = {\n",
    "        'R2': r2_score(y_test, y_test_pred),\n",
    "        'MAE': mean_absolute_error(y_test, y_test_pred),\n",
    "        'MSE': mean_squared_error(y_test, y_test_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "        'R': np.corrcoef(y_test, y_test_pred)[0, 1]\n",
    "    }\n",
    "\n",
    "    return rf_model, train_metrics, test_metrics, y_train_pred, y_test_pred\n",
    "\n",
    "best_r2 = -float('inf')\n",
    "best_train_metrics = None\n",
    "best_test_metrics = None\n",
    "best_y_train_pred = None\n",
    "best_y_test_pred = None\n",
    "best_y_train_actual = None\n",
    "best_y_test_actual = None\n",
    "best_rf_model = None\n",
    "\n",
    "lag = lag n_estimators = 100\n",
    "max_depth = 3\n",
    "\n",
    "X_train, y_train, X_test, y_test, scaler_y = prepare_data_with_lag(data)\n",
    "\n",
    "for i in range(20):\n",
    "    print(f\"Run {i + 1}/20\")\n",
    "    \n",
    "        rf_model, train_metrics, test_metrics, y_train_pred, y_test_pred = train_and_evaluate_rf(\n",
    "        X_train, y_train, X_test, y_test, n_estimators=n_estimators, max_depth=max_depth\n",
    "    )\n",
    "\n",
    "        if test_metrics['R2'] > best_r2:\n",
    "        best_r2 = test_metrics['R2']\n",
    "        best_train_metrics = train_metrics\n",
    "        best_test_metrics = test_metrics\n",
    "        best_y_train_pred = y_train_pred\n",
    "        best_y_test_pred = y_test_pred\n",
    "        best_y_train_actual = y_train\n",
    "        best_y_test_actual = y_test\n",
    "        best_rf_model = rf_model\n",
    "\n",
    "best_y_train_pred = scaler_y.inverse_transform(best_y_train_pred.reshape(-1, 1)).flatten()\n",
    "best_y_test_pred = scaler_y.inverse_transform(best_y_test_pred.reshape(-1, 1)).flatten()\n",
    "best_y_train_actual = scaler_y.inverse_transform(best_y_train_actual.reshape(-1, 1)).flatten()\n",
    "best_y_test_actual = scaler_y.inverse_transform(best_y_test_actual.reshape(-1, 1)).flatten()\n",
    "\n",
    "metrics_test_df = pd.DataFrame(columns=['R2', 'R', 'MSE', 'RMSE', 'MAE'])\n",
    "metrics_train_df = pd.DataFrame(columns=['R2', 'R', 'MSE', 'RMSE', 'MAE'])\n",
    "\n",
    "best_y_preds_test = {'Actual': best_y_test_actual.flatten(), 'Predicted': best_y_test_pred.flatten()}\n",
    "best_y_preds_train = {'Actual': best_y_train_actual.flatten(), 'Predicted': best_y_train_pred.flatten()}\n",
    "\n",
    "metrics_test_df = pd.concat([metrics_test_df, pd.DataFrame([best_test_metrics])], ignore_index=True)\n",
    "metrics_train_df = pd.concat([metrics_train_df, pd.DataFrame([best_train_metrics])], ignore_index=True)\n",
    "\n",
    "summary_df = pd.concat([\n",
    "    pd.DataFrame({'Data': 'Train', 'R2': best_train_metrics['R2'], 'R': best_train_metrics['R'], \n",
    "                  'MSE': best_train_metrics['MSE'], 'RMSE': best_train_metrics['RMSE'], 'MAE': best_train_metrics['MAE']}, index=[0]),\n",
    "    pd.DataFrame({'Data': 'Test', 'R2': best_test_metrics['R2'], 'R': best_test_metrics['R'], \n",
    "                  'MSE': best_test_metrics['MSE'], 'RMSE': best_test_metrics['RMSE'], 'MAE': best_test_metrics['MAE']}, index=[1])\n",
    "])\n",
    "\n",
    "RF_Train = best_y_train_pred.flatten()\n",
    "RF_Test = best_y_test_pred.flatten()\n",
    "\n",
    "with pd.ExcelWriter('RF.xlsx') as writer:\n",
    "    metrics_test_df.to_excel(writer, sheet_name='Test Metrics', index=False)\n",
    "    metrics_train_df.to_excel(writer, sheet_name='Train Metrics', index=False)\n",
    "    pd.DataFrame(best_y_preds_test).to_excel(writer, sheet_name='Test Predictions', index=False)\n",
    "    pd.DataFrame(best_y_preds_train).to_excel(writer, sheet_name='Train Predictions', index=False)\n",
    "    summary_df.to_excel(writer, sheet_name='Metrics Summary', index=False)\n",
    "\n",
    "print(\"Process completed and results saved to 'Best_RF_South_LagFeatures.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d29b6c9-a4c1-4cb0-b3cf-ef6579015cb6",
   "metadata": {},
   "source": [
    "### RF Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d9e353-f60a-4cec-917d-397c60906a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def calculate_month_cyclic_features(date):\n",
    "    month = date.month\n",
    "    month_sin = np.sin(2 * np.pi * month / 12)\n",
    "    month_cos = np.cos(2 * np.pi * month / 12)\n",
    "    return month_sin, month_cos\n",
    "\n",
    "def prepare_forecast_input_rf(last_lag_values, forecast_dates, i, lag=lag):\n",
    "        month_sin, month_cos = calculate_month_cyclic_features(forecast_dates[i])\n",
    "\n",
    "        forecast_input = np.hstack([last_lag_values, month_sin, month_cos])\n",
    "\n",
    "    return forecast_input\n",
    "\n",
    "def forecast_next_records_rf(rf_model, data, lag=lag, forecast_steps=1):\n",
    "        last_lag_values = data[[f'yt-{i}' for i in range(1, lag + 1)]].values[-1]\n",
    "\n",
    "        last_date = data.index[-1]\n",
    "    forecast_dates = [last_date + pd.DateOffset(months=i) for i in range(1, forecast_steps + 1)]\n",
    "\n",
    "        forecast_values = []\n",
    "\n",
    "        for i in range(forecast_steps):\n",
    "                forecast_input = prepare_forecast_input_rf(last_lag_values, forecast_dates, i, lag=lag)\n",
    "\n",
    "                forecast_value_scaled = rf_model.predict(forecast_input.reshape(1, -1))[0]\n",
    "\n",
    "                scaler_y = StandardScaler()\n",
    "        scaler_y.fit(data[['chl_a']].values)\n",
    "        forecast_value = scaler_y.inverse_transform([[forecast_value_scaled]]).flatten()[0]\n",
    "\n",
    "                forecast_values.append(forecast_value)\n",
    "\n",
    "                last_lag_values = np.roll(last_lag_values, -1)          last_lag_values[-1] = forecast_value  \n",
    "    return forecast_values, forecast_dates\n",
    "\n",
    "\n",
    "RF_forecast_values, forecast_dates = forecast_next_records_rf(best_rf_model, data, lag=lag, forecast_steps=forecast_steps)\n",
    "\n",
    "for date, value in zip(forecast_dates, RF_forecast_values):\n",
    "    print(f\"Forecasted Chl-a concentration for {date.date()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cacf6d-6ab9-4651-bdfb-fe8c8ab788ca",
   "metadata": {},
   "source": [
    "# GPR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c0cc9c-8cfd-4217-979f-5987d4fe828d",
   "metadata": {},
   "source": [
    "### Hyperparameters Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a033ad-7fa6-4191-8d07-4632641f9639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ExpSineSquared, WhiteKernel\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "def train_and_evaluate_gpr(X_train, y_train, X_test, y_test, kernel):\n",
    "    gpr_model = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha=1e-5, random_state=42)\n",
    "\n",
    "        gpr_model.fit(X_train, y_train)\n",
    "\n",
    "        y_test_pred = gpr_model.predict(X_test)\n",
    "\n",
    "        r2 = r2_score(y_test, y_test_pred)\n",
    "    return r2, gpr_model\n",
    "\n",
    "lag = lag\n",
    "X_train, y_train, X_test, y_test, scaler_y = prepare_data_with_lag(data)\n",
    "\n",
    "param_grid = {\n",
    "    'kernel': [\n",
    "        1.0 * RBF(length_scale=1.0) + WhiteKernel(noise_level=1.0),\n",
    "        1.0 * ExpSineSquared(length_scale=1.0, periodicity=12.0) + WhiteKernel(noise_level=1.0),\n",
    "        1.0 * RBF(length_scale=2.0) + WhiteKernel(noise_level=1.0),\n",
    "        1.0 * ExpSineSquared(length_scale=2.0, periodicity=12.0) + WhiteKernel(noise_level=1.0),\n",
    "        1.0 * RBF(length_scale=1.0) + ExpSineSquared(length_scale=1.0, periodicity=12.0) + WhiteKernel(noise_level=1.0)\n",
    "    ]\n",
    "}\n",
    "\n",
    "best_r2 = -float('inf')\n",
    "best_params = None\n",
    "best_gpr_model = None\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"Testing kernel: {params['kernel']}\")\n",
    "    r2, gpr_model = train_and_evaluate_gpr(X_train, y_train, X_test, y_test, kernel=params['kernel'])\n",
    "\n",
    "        if r2 > best_r2:\n",
    "        best_r2 = r2\n",
    "        best_params = params\n",
    "        best_gpr_model = gpr_model\n",
    "\n",
    "print(f\"Best kernel: {best_params['kernel']}\")\n",
    "print(f\"Best R2 score: {best_r2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f58a1d-23de-4686-9d41-c78cacfd23cc",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085162c1-8ba4-4e69-a38e-592fd0c9e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ExpSineSquared, WhiteKernel\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def prepare_data_with_lag(data):\n",
    "    X = data[[f'yt-{i}' for i in range(1, lag + 1)] + ['month_sin', 'month_cos']].values\n",
    "    y = data['chl_a'].values\n",
    "    \n",
    "        scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    \n",
    "    X = scaler_X.fit_transform(X)\n",
    "    y = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "    \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, scaler_y\n",
    "\n",
    "def train_and_evaluate_gpr(X_train, y_train, X_test, y_test, kernel):\n",
    "    gpr_model = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha=1e-5, random_state=42)\n",
    "\n",
    "        gpr_model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = gpr_model.predict(X_train)\n",
    "    y_test_pred = gpr_model.predict(X_test)\n",
    "\n",
    "        train_metrics = {\n",
    "        'R2': r2_score(y_train, y_train_pred),\n",
    "        'MAE': mean_absolute_error(y_train, y_train_pred),\n",
    "        'MSE': mean_squared_error(y_train, y_train_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "        'R': np.corrcoef(y_train, y_train_pred)[0, 1]\n",
    "    }\n",
    "\n",
    "    test_metrics = {\n",
    "        'R2': r2_score(y_test, y_test_pred),\n",
    "        'MAE': mean_absolute_error(y_test, y_test_pred),\n",
    "        'MSE': mean_squared_error(y_test, y_test_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "        'R': np.corrcoef(y_test, y_test_pred)[0, 1]\n",
    "    }\n",
    "\n",
    "    return gpr_model, train_metrics, test_metrics, y_train_pred, y_test_pred\n",
    "\n",
    "best_params = {\n",
    "    'kernel': 1.0 * RBF(length_scale=1.0) + ExpSineSquared(length_scale=1.0, periodicity=12.0) + WhiteKernel(noise_level=1.0)\n",
    "}\n",
    "\n",
    "lag = lag\n",
    "X_train, y_train, X_test, y_test, scaler_y = prepare_data_with_lag(data)\n",
    "\n",
    "best_r2 = -float('inf')\n",
    "best_train_metrics = None\n",
    "best_test_metrics = None\n",
    "best_y_train_pred = None\n",
    "best_y_test_pred = None\n",
    "best_y_train_actual = None\n",
    "best_y_test_actual = None\n",
    "best_gpr_model = None\n",
    "\n",
    "for i in range(1):\n",
    "    print(f\"Run {i + 1}/20\")\n",
    "    \n",
    "        gpr_model, train_metrics, test_metrics, y_train_pred, y_test_pred = train_and_evaluate_gpr(\n",
    "        X_train, y_train, X_test, y_test, kernel=best_params['kernel']\n",
    "    )\n",
    "\n",
    "        if test_metrics['R2'] > best_r2:\n",
    "        best_r2 = test_metrics['R2']\n",
    "        best_train_metrics = train_metrics\n",
    "        best_test_metrics = test_metrics\n",
    "        best_y_train_pred = y_train_pred\n",
    "        best_y_test_pred = y_test_pred\n",
    "        best_y_train_actual = y_train\n",
    "        best_y_test_actual = y_test\n",
    "        best_gpr_model = gpr_model  \n",
    "best_y_train_pred = scaler_y.inverse_transform(best_y_train_pred.reshape(-1, 1)).flatten()\n",
    "best_y_test_pred = scaler_y.inverse_transform(best_y_test_pred.reshape(-1, 1)).flatten()\n",
    "best_y_train_actual = scaler_y.inverse_transform(best_y_train_actual.reshape(-1, 1)).flatten()\n",
    "best_y_test_actual = scaler_y.inverse_transform(best_y_test_actual.reshape(-1, 1)).flatten()\n",
    "\n",
    "metrics_test_df = pd.DataFrame(columns=['R2', 'R', 'MSE', 'RMSE', 'MAE'])\n",
    "metrics_train_df = pd.DataFrame(columns=['R2', 'R', 'MSE', 'RMSE', 'MAE'])\n",
    "\n",
    "best_y_preds_test = {'Actual': best_y_test_actual.flatten(), 'Predicted': best_y_test_pred.flatten()}\n",
    "best_y_preds_train = {'Actual': best_y_train_actual.flatten(), 'Predicted': best_y_train_pred.flatten()}\n",
    "\n",
    "metrics_test_df = pd.concat([metrics_test_df, pd.DataFrame([best_test_metrics])], ignore_index=True)\n",
    "metrics_train_df = pd.concat([metrics_train_df, pd.DataFrame([best_train_metrics])], ignore_index=True)\n",
    "\n",
    "summary_df = pd.concat([\n",
    "    pd.DataFrame({'Data': 'Train', 'R2': best_train_metrics['R2'], 'R': best_train_metrics['R'], \n",
    "                  'MSE': best_train_metrics['MSE'], 'RMSE': best_train_metrics['RMSE'], 'MAE': best_train_metrics['MAE']}, index=[0]),\n",
    "    pd.DataFrame({'Data': 'Test', 'R2': best_test_metrics['R2'], 'R': best_test_metrics['R'], \n",
    "                  'MSE': best_test_metrics['MSE'], 'RMSE': best_test_metrics['RMSE'], 'MAE': best_test_metrics['MAE']}, index=[1])\n",
    "])\n",
    "\n",
    "GPR_Train = best_y_train_pred.flatten()\n",
    "GPR_Test = best_y_test_pred.flatten()\n",
    "\n",
    "\n",
    "with pd.ExcelWriter('GPR.xlsx') as writer:\n",
    "    metrics_test_df.to_excel(writer, sheet_name='Test Metrics', index=False)\n",
    "    metrics_train_df.to_excel(writer, sheet_name='Train Metrics', index=False)\n",
    "    pd.DataFrame(best_y_preds_test).to_excel(writer, sheet_name='Test Predictions', index=False)\n",
    "    pd.DataFrame(best_y_preds_train).to_excel(writer, sheet_name='Train Predictions', index=False)\n",
    "    summary_df.to_excel(writer, sheet_name='Metrics Summary', index=False)\n",
    "\n",
    "print(\"Process completed and results saved to 'Best_GPR_South_LagFeatures.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb28b8-ba7f-49c6-8007-b07be1cd5071",
   "metadata": {},
   "source": [
    "### GPR Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23958661-0e0c-40cf-862b-42729c5ebc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def calculate_month_cyclic_features(date):\n",
    "    month = date.month\n",
    "    month_sin = np.sin(2 * np.pi * month / 12)\n",
    "    month_cos = np.cos(2 * np.pi * month / 12)\n",
    "    return month_sin, month_cos\n",
    "\n",
    "def prepare_forecast_input_gpr(last_lag_values, forecast_dates, i, lag=lag):\n",
    "        month_sin, month_cos = calculate_month_cyclic_features(forecast_dates[i])\n",
    "\n",
    "        forecast_input = np.hstack([last_lag_values, month_sin, month_cos])\n",
    "\n",
    "    return forecast_input\n",
    "\n",
    "def forecast_next_records_gpr(gpr_model, data, lag=lag, forecast_steps=1, scaler_y=None):\n",
    "        last_lag_values = data['chl_a'].values[-lag:].flatten()\n",
    "\n",
    "        last_date = data.index[-1]\n",
    "    forecast_dates = [last_date + pd.DateOffset(months=i) for i in range(1, forecast_steps + 1)]\n",
    "\n",
    "        forecast_values = []\n",
    "\n",
    "        for i in range(forecast_steps):\n",
    "                forecast_input = prepare_forecast_input_gpr(last_lag_values, forecast_dates, i, lag=lag)\n",
    "\n",
    "                forecast_value_scaled = gpr_model.predict(forecast_input.reshape(1, -1))[0]\n",
    "\n",
    "                forecast_value = scaler_y.inverse_transform([[forecast_value_scaled]]).flatten()[0]\n",
    "\n",
    "                forecast_values.append(forecast_value)\n",
    "\n",
    "                last_lag_values = np.roll(last_lag_values, -1)          last_lag_values[-1] = forecast_value  \n",
    "    return forecast_values, forecast_dates\n",
    "\n",
    "\n",
    "GPR_forecast_values, forecast_dates = forecast_next_records_gpr(best_gpr_model, data, lag=lag, forecast_steps=forecast_steps, scaler_y=scaler_y)\n",
    "\n",
    "for date, value in zip(forecast_dates, GPR_forecast_values):\n",
    "    print(f\"Forecasted Chl-a concentration for {date.date()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cef7ba-7247-44aa-b3c1-1b523f24d6a5",
   "metadata": {},
   "source": [
    "# SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9843e11-1faf-44b9-9ee7-0d4cd792e986",
   "metadata": {},
   "source": [
    "### Hyperparameters Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b179733-373b-4f1f-a606-ef457ec981a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "def train_and_evaluate_svr(X_train, y_train, X_test, y_test, kernel='rbf', C=1.0, epsilon=0.1):\n",
    "    svr_model = SVR(kernel=kernel, C=C, epsilon=epsilon)\n",
    "\n",
    "        svr_model.fit(X_train, y_train)\n",
    "\n",
    "        y_test_pred = svr_model.predict(X_test)\n",
    "\n",
    "        r2 = r2_score(y_test, y_test_pred)\n",
    "    return r2, svr_model\n",
    "\n",
    "\n",
    "lag = lag\n",
    "X_train, y_train, X_test, y_test, scaler_y = prepare_data_with_lag(data)\n",
    "\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'epsilon': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "best_r2 = -float('inf')\n",
    "best_params = None\n",
    "best_svr_model = None\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"Testing parameters: {params}\")\n",
    "    r2, svr_model = train_and_evaluate_svr(\n",
    "        X_train, y_train, X_test, y_test, \n",
    "        kernel=params['kernel'], \n",
    "        C=params['C'], \n",
    "        epsilon=params['epsilon']\n",
    "    )\n",
    "\n",
    "        if r2 > best_r2:\n",
    "        best_r2 = r2\n",
    "        best_params = params\n",
    "        best_svr_model = svr_model\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best R2 score: {best_r2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd6d6a4-c641-4436-b370-627adf6c3d9c",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69e07bc-fbe6-4105-9949-6d2d0866d3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def prepare_data_with_lag(data):\n",
    "    X = data[[f'yt-{i}' for i in range(1, lag + 1)] + ['month_sin', 'month_cos']].values\n",
    "    y = data['chl_a'].values\n",
    "    \n",
    "        scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    \n",
    "    X = scaler_X.fit_transform(X)\n",
    "    y = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "    \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, scaler_y\n",
    "\n",
    "def train_and_evaluate_svr(X_train, y_train, X_test, y_test, kernel='rbf', C=1.0, epsilon=0.1):\n",
    "    svr_model = SVR(kernel=kernel, C=C, epsilon=epsilon)\n",
    "\n",
    "        svr_model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = svr_model.predict(X_train)\n",
    "    y_test_pred = svr_model.predict(X_test)\n",
    "\n",
    "        train_metrics = {\n",
    "        'R2': r2_score(y_train, y_train_pred),\n",
    "        'MAE': mean_absolute_error(y_train, y_train_pred),\n",
    "        'MSE': mean_squared_error(y_train, y_train_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "        'R': np.corrcoef(y_train, y_train_pred)[0, 1]\n",
    "    }\n",
    "\n",
    "    test_metrics = {\n",
    "        'R2': r2_score(y_test, y_test_pred),\n",
    "        'MAE': mean_absolute_error(y_test, y_test_pred),\n",
    "        'MSE': mean_squared_error(y_test, y_test_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "        'R': np.corrcoef(y_test, y_test_pred)[0, 1]\n",
    "    }\n",
    "\n",
    "    return svr_model, train_metrics, test_metrics, y_train_pred, y_test_pred\n",
    "\n",
    "best_params = {\n",
    "    'kernel': 'rbf',      'C': 1.0,             'epsilon': 0.1    }\n",
    "\n",
    "lag = lag\n",
    "X_train, y_train, X_test, y_test, scaler_y = prepare_data_with_lag(data)\n",
    "\n",
    "best_r2 = -float('inf')\n",
    "best_train_metrics = None\n",
    "best_test_metrics = None\n",
    "best_y_train_pred = None\n",
    "best_y_test_pred = None\n",
    "best_y_train_actual = None\n",
    "best_y_test_actual = None\n",
    "best_svr_model = None  \n",
    "for i in range(20):\n",
    "    print(f\"Run {i + 1}/20\")\n",
    "    \n",
    "        svr_model, train_metrics, test_metrics, y_train_pred, y_test_pred = train_and_evaluate_svr(\n",
    "        X_train, y_train, X_test, y_test, \n",
    "        kernel=best_params['kernel'], \n",
    "        C=best_params['C'], \n",
    "        epsilon=best_params['epsilon']\n",
    "    )\n",
    "\n",
    "        if test_metrics['R2'] > best_r2:\n",
    "        best_r2 = test_metrics['R2']\n",
    "        best_train_metrics = train_metrics\n",
    "        best_test_metrics = test_metrics\n",
    "        best_y_train_pred = y_train_pred\n",
    "        best_y_test_pred = y_test_pred\n",
    "        best_y_train_actual = y_train\n",
    "        best_y_test_actual = y_test\n",
    "        best_svr_model = svr_model  \n",
    "best_y_train_pred = scaler_y.inverse_transform(best_y_train_pred.reshape(-1, 1)).flatten()\n",
    "best_y_test_pred = scaler_y.inverse_transform(best_y_test_pred.reshape(-1, 1)).flatten()\n",
    "best_y_train_actual = scaler_y.inverse_transform(best_y_train_actual.reshape(-1, 1)).flatten()\n",
    "best_y_test_actual = scaler_y.inverse_transform(best_y_test_actual.reshape(-1, 1)).flatten()\n",
    "\n",
    "metrics_test_df = pd.DataFrame(columns=['R2', 'R', 'MSE', 'RMSE', 'MAE'])\n",
    "metrics_train_df = pd.DataFrame(columns=['R2', 'R', 'MSE', 'RMSE', 'MAE'])\n",
    "\n",
    "\n",
    "best_y_preds_test = {'Actual': best_y_test_actual.flatten(), 'Predicted': best_y_test_pred.flatten()}\n",
    "best_y_preds_train = {'Actual': best_y_train_actual.flatten(), 'Predicted': best_y_train_pred.flatten()}\n",
    "\n",
    "metrics_test_df = pd.concat([metrics_test_df, pd.DataFrame([best_test_metrics])], ignore_index=True)\n",
    "metrics_train_df = pd.concat([metrics_train_df, pd.DataFrame([best_train_metrics])], ignore_index=True)\n",
    "\n",
    "summary_df = pd.concat([\n",
    "    pd.DataFrame({'Data': 'Train', 'R2': best_train_metrics['R2'], 'R': best_train_metrics['R'], \n",
    "                  'MSE': best_train_metrics['MSE'], 'RMSE': best_train_metrics['RMSE'], 'MAE': best_train_metrics['MAE']}, index=[0]),\n",
    "    pd.DataFrame({'Data': 'Test', 'R2': best_test_metrics['R2'], 'R': best_test_metrics['R'], \n",
    "                  'MSE': best_test_metrics['MSE'], 'RMSE': best_test_metrics['RMSE'], 'MAE': best_test_metrics['MAE']}, index=[1])\n",
    "])\n",
    "\n",
    "SVR_Train = best_y_train_pred.flatten()\n",
    "SVR_Test = best_y_test_pred.flatten()\n",
    "\n",
    "with pd.ExcelWriter('SVR.xlsx') as writer:\n",
    "    metrics_test_df.to_excel(writer, sheet_name='Test Metrics', index=False)\n",
    "    metrics_train_df.to_excel(writer, sheet_name='Train Metrics', index=False)\n",
    "    pd.DataFrame(best_y_preds_test).to_excel(writer, sheet_name='Test Predictions', index=False)\n",
    "    pd.DataFrame(best_y_preds_train).to_excel(writer, sheet_name='Train Predictions', index=False)\n",
    "    summary_df.to_excel(writer, sheet_name='Metrics Summary', index=False)\n",
    "\n",
    "print(\"Process completed and results saved to 'Best_SVR_South_LagFeatures_Cyclic.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badaa12c-86c1-43c1-b844-29c58415a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_month_cyclic_features(date):\n",
    "    month = date.month\n",
    "    month_sin = np.sin(2 * np.pi * month / 12)\n",
    "    month_cos = np.cos(2 * np.pi * month / 12)\n",
    "    return month_sin, month_cos\n",
    "\n",
    "def prepare_forecast_input_svr(last_lag_values, forecast_dates, i, lag=lag):\n",
    "        month_sin, month_cos = calculate_month_cyclic_features(forecast_dates[i])\n",
    "\n",
    "        forecast_input = np.hstack([last_lag_values, month_sin, month_cos])\n",
    "\n",
    "    return forecast_input\n",
    "\n",
    "def forecast_next_records_svr(svr_model, data, lag=lag, forecast_steps=1, scaler_y=None):\n",
    "        if not pd.api.types.is_datetime64_any_dtype(data.index):\n",
    "        data.index = pd.to_datetime(data.index)\n",
    "\n",
    "        last_lag_values = data['chl_a'].values[-lag:].flatten()\n",
    "\n",
    "        last_date = data.index[-1]\n",
    "    forecast_dates = [last_date + pd.DateOffset(months=i) for i in range(1, forecast_steps + 1)]\n",
    "\n",
    "        forecast_values = []\n",
    "\n",
    "        for i in range(forecast_steps):\n",
    "                forecast_input = prepare_forecast_input_svr(last_lag_values, forecast_dates, i, lag=lag)\n",
    "\n",
    "                forecast_value_scaled = svr_model.predict(forecast_input.reshape(1, -1))[0]\n",
    "\n",
    "                forecast_value = scaler_y.inverse_transform([[forecast_value_scaled]]).flatten()[0]\n",
    "\n",
    "                forecast_values.append(forecast_value)\n",
    "\n",
    "                last_lag_values = np.roll(last_lag_values, -1)          last_lag_values[-1] = forecast_value  \n",
    "    return forecast_values, forecast_dates\n",
    "\n",
    "forecast_steps = 6  \n",
    "SVR_forecast_values, forecast_dates = forecast_next_records_svr(best_svr_model, data, lag=lag, forecast_steps=forecast_steps, scaler_y=scaler_y)\n",
    "\n",
    "for date, value in zip(forecast_dates, SVR_forecast_values):\n",
    "    print(f\"Forecasted Chl-a concentration for {date.date()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa2be5c-d12a-4ee7-80a6-f98e421ecdbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c84b90-68cb-44e8-8ea0-3005addbbd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec26b99-e733-4a59-9de7-dd3f5202b33c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "'''import numpy as np\n",
    "import torch\n",
    "from kan import KAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def custom_r2(y_true, y_pred):\n",
    "    y_true_mean = np.mean(y_true)\n",
    "    y_pred_mean = np.mean(y_pred)\n",
    "    numerator = np.mean((y_true - y_true_mean) * (y_pred - y_pred_mean))\n",
    "    denominator = np.sqrt(np.mean((y_true - y_true_mean) ** 2) * np.mean((y_pred - y_pred_mean) ** 2))\n",
    "    r = numerator / denominator\n",
    "    return r ** 2\n",
    "\n",
    "def load_chla_dataset(data, lag=3):\n",
    "    target = data['chl_a'].values\n",
    "    input_data = data[[f'yt-{i}' for i in range(1, lag + 1)] + ['month_sin', 'month_cos']].values\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "    input_data = scaler.fit_transform(input_data)\n",
    "\n",
    "        input_tensor = torch.tensor(input_data, dtype=torch.float32, device=device)\n",
    "    target_tensor = torch.tensor(target, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "\n",
    "        train_data, test_data, train_target, test_target = train_test_split(input_tensor, target_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "        return {\n",
    "        'train_input': train_data,\n",
    "        'train_label': train_target,\n",
    "        'test_input': test_data,\n",
    "        'test_label': test_target\n",
    "    }\n",
    "\n",
    "\n",
    "def build_and_train_kan_model(dataset):\n",
    "    KAN_model = KAN(\n",
    "        width=[dataset['train_input'].shape[1], 7, 1],\n",
    "        grid=3,\n",
    "        k=3,\n",
    "        noise_scale=0.1,\n",
    "        noise_scale_base=0.1,\n",
    "        symbolic_enabled=False,\n",
    "        bias_trainable=False,\n",
    "        grid_eps=1.0,\n",
    "        grid_range=[-1, 1],\n",
    "        sp_trainable=True,\n",
    "        sb_trainable=True,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "        def train_mse():\n",
    "        with torch.no_grad():\n",
    "            predictions = KAN_model(dataset['train_input'])\n",
    "            return torch.nn.functional.mse_loss(predictions, dataset['train_label'])\n",
    "\n",
    "    def test_mse():\n",
    "        with torch.no_grad():\n",
    "            predictions = KAN_model(dataset['test_input'])\n",
    "            return torch.nn.functional.mse_loss(predictions, dataset['test_label'])\n",
    "\n",
    "        results = KAN_model.train(\n",
    "        dataset, opt=\"Adam\", device=device, metrics=(train_mse, test_mse),\n",
    "        steps=100,\n",
    "        log=1,\n",
    "        lamb=0.01,\n",
    "        lamb_l1=0.1,\n",
    "        lamb_entropy=0.01,\n",
    "        lamb_coef=1,\n",
    "        lamb_coefdiff=0.1,\n",
    "        update_grid=True,\n",
    "        grid_update_num=10,\n",
    "        loss_fn=torch.nn.SmoothL1Loss(),\n",
    "        lr=0.01,\n",
    "        stop_grid_update_step=50,\n",
    "        batch=-1,\n",
    "        small_mag_threshold=1e-16,\n",
    "        small_reg_factor=0.1,\n",
    "        sglr_avoid=False\n",
    "    )\n",
    "\n",
    "        with torch.no_grad():\n",
    "        y_pred_train = KAN_model(dataset['train_input']).cpu().numpy().flatten()\n",
    "        y_pred_test = KAN_model(dataset['test_input']).cpu().numpy().flatten()\n",
    "\n",
    "        train_metrics = {\n",
    "        'R2': r2_score(dataset['train_label'].cpu().numpy(), y_pred_train),\n",
    "        'MAE': mean_absolute_error(dataset['train_label'].cpu().numpy(), y_pred_train),\n",
    "        'MSE': mean_squared_error(dataset['train_label'].cpu().numpy(), y_pred_train),\n",
    "        'RMSE': np.sqrt(mean_squared_error(dataset['train_label'].cpu().numpy(), y_pred_train)),\n",
    "        'R': np.corrcoef(dataset['train_label'].cpu().numpy().flatten(), y_pred_train)[0, 1]\n",
    "    }\n",
    "\n",
    "    test_metrics = {\n",
    "        'R2': r2_score(dataset['test_label'].cpu().numpy(), y_pred_test),\n",
    "        'MAE': mean_absolute_error(dataset['test_label'].cpu().numpy(), y_pred_test),\n",
    "        'MSE': mean_squared_error(dataset['test_label'].cpu().numpy(), y_pred_test),\n",
    "        'RMSE': np.sqrt(mean_squared_error(dataset['test_label'].cpu().numpy(), y_pred_test)),\n",
    "        'R': np.corrcoef(dataset['test_label'].cpu().numpy().flatten(), y_pred_test)[0, 1]\n",
    "    }\n",
    "\n",
    "    return KAN_model, train_metrics, test_metrics, y_pred_train, y_pred_test\n",
    "\n",
    "\n",
    "metrics_test_df = pd.DataFrame(columns=['R2', 'R', 'MSE', 'RMSE', 'MAE'])\n",
    "metrics_train_df = pd.DataFrame(columns=['R2', 'R', 'MSE', 'RMSE', 'MAE'])\n",
    "\n",
    "best_y_preds_test = {}\n",
    "best_y_preds_train = {}\n",
    "\n",
    "\n",
    "chla_dataset = load_chla_dataset(data, lag=lag)\n",
    "\n",
    "KAN_model, train_metrics, test_metrics, y_pred_train, y_pred_test = build_and_train_kan_model(chla_dataset)\n",
    "\n",
    "metrics_test_df = pd.concat([metrics_test_df, pd.DataFrame([test_metrics])], ignore_index=True)\n",
    "metrics_train_df = pd.concat([metrics_train_df, pd.DataFrame([train_metrics])], ignore_index=True)\n",
    "\n",
    "best_y_preds_test['Actual'] = chla_dataset['test_label'].cpu().numpy().flatten()\n",
    "best_y_preds_test['Predicted'] = y_pred_test\n",
    "\n",
    "best_y_preds_train['Actual'] = chla_dataset['train_label'].cpu().numpy().flatten()\n",
    "best_y_preds_train['Predicted'] = y_pred_train\n",
    "\n",
    "summary_df = pd.concat([\n",
    "    pd.DataFrame({'Data': 'Train', 'R2': train_metrics['R2'], 'R': train_metrics['R'], \n",
    "                  'MSE': train_metrics['MSE'], 'RMSE': train_metrics['RMSE'], 'MAE': train_metrics['MAE']}, index=[0]),\n",
    "    pd.DataFrame({'Data': 'Test', 'R2': test_metrics['R2'], 'R': test_metrics['R'], \n",
    "                  'MSE': test_metrics['MSE'], 'RMSE': test_metrics['RMSE'], 'MAE': test_metrics['MAE']}, index=[1])\n",
    "])\n",
    "\n",
    "with pd.ExcelWriter('KAN.xlsx') as writer:\n",
    "    metrics_test_df.to_excel(writer, sheet_name='Test Metrics', index=False)\n",
    "    metrics_train_df.to_excel(writer, sheet_name='Train Metrics', index=False)\n",
    "    pd.DataFrame(best_y_preds_test).to_excel(writer, sheet_name='Test Predictions', index=False)\n",
    "    pd.DataFrame(best_y_preds_train).to_excel(writer, sheet_name='Train Predictions', index=False)\n",
    "    summary_df.to_excel(writer, sheet_name='Metrics Summary', index=False)\n",
    "\n",
    "print(\"Process completed and results saved to 'KAN_South.xlsx'\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f28fe4a-967a-40de-a606-4db575ea9549",
   "metadata": {},
   "source": [
    "# TEST  DO NOT USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cc75dc-f900-495d-9cda-7251a40ae854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from tkan import TKAN\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "BACKEND = 'jax'\n",
    "os.environ['KERAS_BACKEND'] = BACKEND\n",
    "\n",
    "keras.utils.set_random_seed(1)\n",
    "\n",
    "N_MAX_EPOCHS = 100\n",
    "BATCH_SIZE = 24\n",
    "\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "lr_callback = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.25,\n",
    "    patience=5,\n",
    "    min_lr=0.00025,\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping_callback, lr_callback]\n",
    "\n",
    "def generate_data(df, sequence_length, n_ahead=1):\n",
    "    X = df[[f'yt-{i}' for i in range(1, sequence_length + 1)] + ['month_sin', 'month_cos']].values\n",
    "    y = df['chl_a'].values\n",
    "\n",
    "        X = X.reshape((X.shape[0], sequence_length, -1))\n",
    "\n",
    "    train_size = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "sequence_length = 1  X_train, X_test, y_train, y_test = generate_data(data_LSTM, sequence_length, 1)\n",
    "\n",
    "TKAN_model = Sequential([\n",
    "    Input(shape=(sequence_length, X_train.shape[2])),\n",
    "    TKAN(100, sub_kan_configs=[{'spline_order': 3, 'grid_size': 5}], return_sequences=False),\n",
    "            Dense(units=1, activation='linear')  ])\n",
    "\n",
    "TKAN_model.compile(optimizer=\"Adam\", loss='mean_squared_error')\n",
    "\n",
    "history = TKAN_model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=N_MAX_EPOCHS, \n",
    "                          validation_split=0.2, shuffle=False)\n",
    "\n",
    "y_train_pred = TKAN_model.predict(X_train).flatten()\n",
    "y_test_pred = TKAN_model.predict(X_test).flatten()\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100      r2 = r2_score(y_true, y_pred)\n",
    "    r, p_value = pearsonr(y_true, y_pred)\n",
    "    return mse, rmse, mae, mape, r2, r, p_value\n",
    "\n",
    "train_metrics = calculate_metrics(y_train, y_train_pred)\n",
    "test_metrics = calculate_metrics(y_test, y_test_pred)\n",
    "\n",
    "train_results_df = pd.DataFrame({\n",
    "    'Actual Train': y_train.flatten(),\n",
    "    'Predicted Train': y_train_pred\n",
    "})\n",
    "\n",
    "test_results_df = pd.DataFrame({\n",
    "    'Actual Test': y_test.flatten(),\n",
    "    'Predicted Test': y_test_pred\n",
    "})\n",
    "\n",
    "def save_to_excel(file_name, train_df, test_df, metrics_df):\n",
    "    with pd.ExcelWriter(file_name) as writer:\n",
    "        train_df.to_excel(writer, sheet_name='Train Results', index=False)\n",
    "        test_df.to_excel(writer, sheet_name='Test Results', index=False)\n",
    "        metrics_df.to_excel(writer, sheet_name='Metrics', index=False)\n",
    "    print(f\"Exported actual vs predicted values and metrics to {file_name}\")\n",
    "\n",
    "TKAN_Train = y_train_pred\n",
    "TKAN_Test = y_test_pred\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['MSE', 'RMSE', 'MAE', 'MAPE', 'R²', 'R', 'P-value'],\n",
    "    'Training': train_metrics,\n",
    "    'Testing': test_metrics\n",
    "})\n",
    "\n",
    "save_to_excel(\"TKAN.xlsx\", train_results_df, test_results_df, metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b0a285-f805-4b73-b531-61f9106917cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TKAN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030b4c74-46aa-400a-a082-bd0e7bc224d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_month_cyclic_features(date):\n",
    "    month = date.month\n",
    "    month_sin = np.sin(2 * np.pi * month / 12)\n",
    "    month_cos = np.cos(2 * np.pi * month / 12)\n",
    "    return month_sin, month_cos\n",
    "\n",
    "def prepare_forecast_input_tkan(last_lag_values, forecast_dates, lag=lag):\n",
    "        forecast_inputs = []\n",
    "    for date in forecast_dates:\n",
    "                month_sin, month_cos = calculate_month_cyclic_features(date)\n",
    "        \n",
    "                forecast_input = np.hstack([last_lag_values.reshape(-1, 1), np.tile([month_sin, month_cos], (lag, 1))])\n",
    "        forecast_inputs.append(forecast_input)\n",
    "    \n",
    "        forecast_inputs = np.array(forecast_inputs)\n",
    "    return forecast_inputs\n",
    "\n",
    "def forecast_next_records_tkan(TKAN_model, data, lag=lag, forecast_steps=12):\n",
    "        last_lag_values = data['chl_a'].values[-lag:].flatten()\n",
    "\n",
    "        last_date = data.index[-1]\n",
    "    forecast_dates = [last_date + pd.DateOffset(months=i) for i in range(1, forecast_steps + 1)]\n",
    "\n",
    "        forecast_input = prepare_forecast_input_tkan(last_lag_values, forecast_dates, lag)\n",
    "\n",
    "        forecast_values_scaled = TKAN_model.predict(forecast_input)\n",
    "\n",
    "        scaler_y = StandardScaler()\n",
    "    scaler_y.fit(data[['chl_a']].values)\n",
    "    forecast_values = scaler_y.inverse_transform(forecast_values_scaled)\n",
    "\n",
    "    return forecast_values.flatten(), forecast_dates\n",
    "\n",
    "forecast_steps = 12  lag = lag  \n",
    "TKAN_forecast_values, forecast_dates = forecast_next_records_tkan(TKAN_model, data_LSTM, lag=lag, forecast_steps=forecast_steps)\n",
    "\n",
    "for date, value in zip(forecast_dates, TKAN_forecast_values):\n",
    "    print(f\"Forecasted Chl-a concentration for {date.date()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df3c67b-ef67-431a-bc88-655181c00f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "forecast_data = {\n",
    "    'Date': forecast_dates,      'Actual': data_for_forecasting['chl_a'].values,      'MLP_Forecast': ANN_forecast_values,\n",
    "    'RF_Forecast': RF_forecast_values,\n",
    "    'LSTM_Forecast': LSTM_forecast_values,\n",
    "    'GRU_Forecast': GRU_forecast_values,\n",
    "    'GPR_Forecast': GPR_forecast_values,\n",
    "    'SVR_Forecast': SVR_forecast_values\n",
    "}\n",
    "\n",
    "forecast_df = pd.DataFrame(forecast_data)\n",
    "\n",
    "forecast_df.to_excel('forecast.xlsx', index=False)\n",
    "\n",
    "print(\"Forecast results have been successfully exported to 'forecast.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8322ad-7c54-4bfc-a432-95bcbec1ef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def calculate_metrics(actual, predicted):\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = mean_squared_error(actual, predicted, squared=False)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100 if np.any(actual) else 0\n",
    "    X = sm.add_constant(predicted)\n",
    "    model = sm.OLS(actual, X).fit()\n",
    "    p_value = model.pvalues[1]\n",
    "    return r2, mse, rmse, mae, mape, p_value\n",
    "\n",
    "train_df = pd.DataFrame({\n",
    "    'Actual': Act_Train,\n",
    "    'ANN': ANN_Train,\n",
    "    'LSTM': LSTM_Train,\n",
    "    'GRU': GRU_Train,\n",
    "    'RF': RF_Train, \n",
    "    'GPR': GPR_Train,\n",
    "    'SVR': SVR_Train\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'Actual': Act_Test,\n",
    "    'ANN': ANN_Test,\n",
    "    'LSTM': LSTM_Test,\n",
    "    'GRU': GRU_Test,\n",
    "    'RF': RF_Test, \n",
    "    'GPR': GPR_Test,\n",
    "    'SVR': SVR_Test\n",
    "})\n",
    "\n",
    "models = ['ANN', 'LSTM', 'GRU', 'RF', 'GPR', 'SVR']\n",
    "\n",
    "def calculate_all_metrics(actual_train, actual_test):\n",
    "    metrics_train = {'Model': [], 'R2': [], 'MSE': [], 'RMSE': [], 'MAE': [], 'MAPE': [], 'p-value': []}\n",
    "    metrics_test = {'Model': [], 'R2': [], 'MSE': [], 'RMSE': [], 'MAE': [], 'MAPE': [], 'p-value': []}\n",
    "    \n",
    "    for model in models:\n",
    "                preds_train = globals()[f\"{model}_Train\"]\n",
    "        metrics_train['Model'].append(model)\n",
    "        metrics_train_values = calculate_metrics(actual_train, preds_train)\n",
    "        metrics_train['R2'].append(metrics_train_values[0])\n",
    "        metrics_train['MSE'].append(metrics_train_values[1])\n",
    "        metrics_train['RMSE'].append(metrics_train_values[2])\n",
    "        metrics_train['MAE'].append(metrics_train_values[3])\n",
    "        metrics_train['MAPE'].append(metrics_train_values[4])\n",
    "        metrics_train['p-value'].append(metrics_train_values[5])\n",
    "\n",
    "                preds_test = globals()[f\"{model}_Test\"]\n",
    "        metrics_test['Model'].append(model)\n",
    "        metrics_test_values = calculate_metrics(actual_test, preds_test)\n",
    "        metrics_test['R2'].append(metrics_test_values[0])\n",
    "        metrics_test['MSE'].append(metrics_test_values[1])\n",
    "        metrics_test['RMSE'].append(metrics_test_values[2])\n",
    "        metrics_test['MAE'].append(metrics_test_values[3])\n",
    "        metrics_test['MAPE'].append(metrics_test_values[4])\n",
    "        metrics_test['p-value'].append(metrics_test_values[5])\n",
    "    \n",
    "    return pd.DataFrame(metrics_train), pd.DataFrame(metrics_test)\n",
    "\n",
    "metrics_train_df, metrics_test_df = calculate_all_metrics(Act_Train, Act_Test)\n",
    "\n",
    "with pd.ExcelWriter('model_predictions_metrics.xlsx') as writer:\n",
    "    train_df.to_excel(writer, sheet_name='Train Actual vs Predicted', index=False)\n",
    "    test_df.to_excel(writer, sheet_name='Test Actual vs Predicted', index=False)\n",
    "    metrics_train_df.to_excel(writer, sheet_name='Train Metrics', index=False)\n",
    "    metrics_test_df.to_excel(writer, sheet_name='Test Metrics', index=False)\n",
    "\n",
    "print(\"Excel file saved with model predictions and metrics.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8469635e-dd05-4d46-a161-0f3721d6ddf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU, Dense, Input\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tkan import TKAN  \n",
    "import time\n",
    "\n",
    "keras.utils.set_random_seed(1)\n",
    "\n",
    "N_MAX_EPOCHS = 1000\n",
    "BATCH_SIZE = 24\n",
    "\n",
    "early_stopping_callback = lambda: keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.00001,\n",
    "    patience=10,\n",
    "    mode=\"min\",\n",
    "    restore_best_weights=True,\n",
    "    start_from_epoch=6,\n",
    ")\n",
    "lr_callback = lambda: keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.25,\n",
    "    patience=5,\n",
    "    mode=\"min\",\n",
    "    min_delta=0.00001,\n",
    "    min_lr=0.000025,\n",
    "    verbose=0,\n",
    ")\n",
    "callbacks = lambda: [\n",
    "    early_stopping_callback(),\n",
    "    lr_callback(),\n",
    "    keras.callbacks.TerminateOnNaN(),\n",
    "]\n",
    "\n",
    "\n",
    "X = data[[f'yt-{i}' for i in range(1, lag + 1)] + ['month_sin', 'month_cos']].values\n",
    "y = data['chl_a'].values\n",
    "\n",
    "def create_sequences(X, y, sequence_length):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - sequence_length):\n",
    "        Xs.append(X[i : (i + sequence_length)])\n",
    "        ys.append(y[i + sequence_length])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "sequence_length = 12  \n",
    "X_seq, y_seq = create_sequences(X, y, sequence_length)\n",
    "\n",
    "train_size = int(len(X_seq) * 0.8)\n",
    "X_train, X_test = X_seq[:train_size], X_seq[train_size:]\n",
    "y_train, y_test = y_seq[:train_size], y_seq[train_size:]\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train.reshape(-1, X_train.shape[2])).reshape(\n",
    "    X_train.shape\n",
    ")\n",
    "X_test_scaled = scaler_X.transform(X_test.reshape(-1, X_test.shape[2])).reshape(\n",
    "    X_test.shape\n",
    ")\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "def build_model(model_type, input_shape):\n",
    "    if model_type == 'TKAN':\n",
    "        model = Sequential(\n",
    "            [\n",
    "                Input(shape=input_shape),\n",
    "                TKAN(32, return_sequences=True),\n",
    "                TKAN(32, sub_kan_output_dim = 15, sub_kan_input_dim = 10, return_sequences=False),\n",
    "                Dense(units=1, activation='linear'),\n",
    "            ],\n",
    "            name=model_type,\n",
    "        )\n",
    "    elif model_type == 'GRU':\n",
    "        model = Sequential(\n",
    "            [\n",
    "                Input(shape=input_shape),\n",
    "                GRU(100, return_sequences=True),\n",
    "                GRU(100, return_sequences=False),\n",
    "                Dense(units=1, activation='relu'),\n",
    "            ],\n",
    "            name=model_type,\n",
    "        )\n",
    "    elif model_type == 'LSTM':\n",
    "        model = Sequential(\n",
    "            [\n",
    "                Input(shape=input_shape),\n",
    "                LSTM(32, return_sequences=True),\n",
    "                LSTM(64, return_sequences=False),\n",
    "                Dense(units=1,activation=None),\n",
    "            ],\n",
    "            name=model_type,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type {model_type}\")\n",
    "    return model\n",
    "\n",
    "models = ['TKAN', 'GRU', 'LSTM'] \n",
    "train_predictions = {}\n",
    "test_predictions = {}\n",
    "train_actuals = None\n",
    "test_actuals = None\n",
    "\n",
    "for model_type in models:\n",
    "    print(f\"\\nTraining {model_type} model...\")\n",
    "    model = build_model(model_type, X_train_scaled.shape[1:])\n",
    "    optimizer = keras.optimizers.Adam(0.001)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss='mean_squared_error', jit_compile=True\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "        start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train_scaled,\n",
    "        y_train_scaled,\n",
    "        validation_split=0.2,\n",
    "        epochs=N_MAX_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks(),\n",
    "                verbose=1,\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "        y_train_pred_scaled = model.predict(X_train_scaled)\n",
    "    y_test_pred_scaled = model.predict(X_test_scaled)\n",
    "\n",
    "        y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled)\n",
    "    y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled)\n",
    "\n",
    "        train_predictions[model_type] = y_train_pred\n",
    "    test_predictions[model_type] = y_test_pred\n",
    "\n",
    "        if train_actuals is None:\n",
    "        y_train_actual = scaler_y.inverse_transform(y_train_scaled)\n",
    "        y_test_actual = scaler_y.inverse_transform(y_test_scaled)\n",
    "        train_actuals = y_train_actual\n",
    "        test_actuals = y_test_actual\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "train_results_df = pd.DataFrame({'Actual': train_actuals.flatten()})\n",
    "test_results_df = pd.DataFrame({'Actual': test_actuals.flatten()})\n",
    "\n",
    "train_metrics = {}\n",
    "test_metrics = {}\n",
    "\n",
    "for model_name in models:\n",
    "    y_train_pred = train_predictions[model_name]\n",
    "    y_test_pred = test_predictions[model_name]\n",
    "\n",
    "        train_results_df[model_name] = y_train_pred.flatten()\n",
    "    test_results_df[model_name] = y_test_pred.flatten()\n",
    "\n",
    "        train_r2 = r2_score(train_actuals, y_train_pred)\n",
    "    train_mse = mean_squared_error(train_actuals, y_train_pred)\n",
    "    train_rmse = np.sqrt(train_mse)\n",
    "    train_mae = np.mean(np.abs(train_actuals - y_train_pred))\n",
    "    train_mape = np.mean(np.abs((train_actuals - y_train_pred) / train_actuals)) * 100\n",
    "    train_r, train_p_value = stats.pearsonr(train_actuals.flatten(), y_train_pred.flatten())\n",
    "\n",
    "        train_metrics[model_name] = {\n",
    "        'R2': train_r2,\n",
    "        'MSE': train_mse,\n",
    "        'RMSE': train_rmse,\n",
    "        'MAE': train_mae,\n",
    "        'MAPE': train_mape,\n",
    "        'R': train_r,\n",
    "        'p-value': train_p_value\n",
    "    }\n",
    "\n",
    "        test_r2 = r2_score(test_actuals, y_test_pred)\n",
    "    test_mse = mean_squared_error(test_actuals, y_test_pred)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    test_mae = np.mean(np.abs(test_actuals - y_test_pred))\n",
    "    test_mape = np.mean(np.abs((test_actuals - y_test_pred) / test_actuals)) * 100\n",
    "    test_r, test_p_value = stats.pearsonr(test_actuals.flatten(), y_test_pred.flatten())\n",
    "\n",
    "        test_metrics[model_name] = {\n",
    "        'R2': test_r2,\n",
    "        'MSE': test_mse,\n",
    "        'RMSE': test_rmse,\n",
    "        'MAE': test_mae,\n",
    "        'MAPE': test_mape,\n",
    "        'R': test_r,\n",
    "        'p-value': test_p_value\n",
    "    }\n",
    "\n",
    "train_metrics_df = pd.DataFrame(train_metrics).T\n",
    "test_metrics_df = pd.DataFrame(test_metrics).T\n",
    "\n",
    "with pd.ExcelWriter('Model_Results.xlsx') as writer:\n",
    "    train_results_df.to_excel(writer, sheet_name='Train Results', index=False)\n",
    "    test_results_df.to_excel(writer, sheet_name='Test Results', index=False)\n",
    "    train_metrics_df.to_excel(writer, sheet_name='Train Metrics')\n",
    "    test_metrics_df.to_excel(writer, sheet_name='Test Metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd842a7-749f-4ce0-9512-fd33b2276778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdcce10-aa52-4909-8d0f-2c7242d1a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_month_cyclic_features(date):\n",
    "    month = date.month\n",
    "    month_sin = np.sin(2 * np.pi * (month - 1) / 12)\n",
    "    month_cos = np.cos(2 * np.pi * (month - 1) / 12)\n",
    "    return month_sin, month_cos\n",
    "\n",
    "forecast_steps = 12\n",
    "lag = lag  \n",
    "last_sequence = data_LSTM[['chl_a', 'month_sin', 'month_cos']].values[-lag:]\n",
    "\n",
    "forecast_dates = pd.date_range(\n",
    "    start=data.index[-1] + pd.DateOffset(months=1),      periods=forecast_steps,\n",
    "    freq='MS'\n",
    ")\n",
    "\n",
    "forecast_df = pd.DataFrame({'Date': forecast_dates})\n",
    "\n",
    "for model_name in models:\n",
    "    print(f\"\\nForecasting with {model_name} model...\")\n",
    "    forecasts = []\n",
    "    last_sequence_copy = last_sequence.copy()\n",
    "\n",
    "        model = build_model(model_name, X_train_scaled.shape[1:])\n",
    "\n",
    "        for i in range(forecast_steps):\n",
    "                print(last_sequence_copy)\n",
    "\n",
    "                forecast_date = forecast_dates[i]\n",
    "\n",
    "                month_sin, month_cos = calculate_month_cyclic_features(forecast_date)\n",
    "\n",
    "                                new_input = last_sequence_copy.copy()\n",
    "\n",
    "                        new_input_full = np.zeros((lag, 14))          new_input_full[:, 0:3] = new_input  \n",
    "                new_input_full[-1, 1] = month_sin          new_input_full[-1, 2] = month_cos  \n",
    "                new_input_scaled = scaler_X.transform(new_input_full)\n",
    "        X_input = new_input_scaled.reshape(1, *new_input_scaled.shape)  \n",
    "                y_forecast_scaled = model.predict(X_input)\n",
    "        y_forecast = scaler_y.inverse_transform(y_forecast_scaled).flatten()[0]\n",
    "\n",
    "                forecasts.append(y_forecast)\n",
    "\n",
    "                        new_row = np.zeros(14)          new_row[0] = y_forecast          new_row[1] = month_sin           new_row[2] = month_cos           \n",
    "                last_sequence_copy = np.vstack([new_input_full[1:], new_row])\n",
    "\n",
    "        forecast_df[model_name] = forecasts\n",
    "\n",
    "with pd.ExcelWriter('Model_Results.xlsx', mode='a', engine='openpyxl') as writer:\n",
    "    forecast_df.to_excel(writer, sheet_name='Forecasts', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5c81a6-ffc3-4114-bf2f-1d2368e824bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fc2a77-342c-459e-a666-7e58d5eba61f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
